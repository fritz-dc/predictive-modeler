{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Donor Modeling Pipeline - Production Ready\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a complete, configurable modeling pipeline for donor behavior prediction:\n",
    "\n",
    "- **Configuration-driven**: All settings in YAML files\n",
    "- **Feature selection**: Per-cohort, per-label automatic selection\n",
    "- **CV grid search**: Adaptive hyperparameter tuning based on label characteristics\n",
    "- **Multiple models**: LogReg, LGBM, Ridge, Tweedie (for revenue)\n",
    "- **Optional ensemble**: Weighted averaging with SHAP on best model\n",
    "- **Threshold optimization**: Find optimal cutoffs for business objectives\n",
    "- **Full explainability**: SHAP values for best model\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Edit **Section 0** (Runtime Configuration) with your label and cohorts\n",
    "2. Run all cells\n",
    "3. Results saved to `./models/<label>/<cohort>/best_model/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 0: Runtime Configuration\n",
    "**Edit this cell for each modeling run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Runtime configuration set\n",
      "  Label: label_will_give_during_giving_tuesday\n",
      "  Cohorts: ['all_donors']\n",
      "  Models: ALL ENABLED\n",
      "  Ensemble: False\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RUNTIME CONFIGURATION - Edit this cell to iterate quickly\n",
    "# ============================================================================\n",
    "\n",
    "# --- Quick Settings (change these frequently) ---\n",
    "LABEL = 'label_will_give_during_giving_tuesday'\n",
    "\n",
    "# Define ALL cohorts in cohorts.yaml, but only run these:\n",
    "COHORTS_TO_RUN = ['all_donors']  # or None for all defined cohorts\n",
    "\n",
    "# Which models to train (None = all enabled in models.yaml)\n",
    "MODELS_TO_RUN = None  # or ['logreg', 'lgbm'] to specify\n",
    "\n",
    "# Enable ensemble?\n",
    "ENABLE_ENSEMBLE = False\n",
    "\n",
    "# Manual feature exclusions (optional)\n",
    "MANUAL_EXCLUDE_FEATURES = [\n",
    "    'days_since_last_email_sent',\n",
    "    'emails_clicked_3m',\n",
    "    'emails_opened_3m',\n",
    "    'emails_clicked_12m',\n",
    "    'emails_opened_12m',\n",
    "    'email_click_rate_3m',\n",
    "    'email_click_rate_12m',\n",
    "    'emails_sent_3m',\n",
    "    'emails_sent_12m',\n",
    "]\n",
    "\n",
    "# --- Data Paths ---\n",
    "TRAIN_FEATURES_PATH = '/Users/matt.fritz/Desktop/train_features_all_last3yr_365lag.csv'\n",
    "OOT_FEATURES_PATH = '/Users/matt.fritz/Desktop/oot_features_all_last3yr_365lag.csv'\n",
    "\n",
    "# --- Config Directory ---\n",
    "CONFIG_DIR = '/Users/matt.fritz/Desktop/modeling_config'\n",
    "\n",
    "# --- Output Settings ---\n",
    "OUTPUT_BASE_DIR = '/Users/matt.fritz/Desktop/modeling_results'\n",
    "SAVE_CANDIDATE_MODELS = False  # Only save best model by default\n",
    "VERBOSE = True\n",
    "\n",
    "print(\"✓ Runtime configuration set\")\n",
    "print(f\"  Label: {LABEL}\")\n",
    "print(f\"  Cohorts: {COHORTS_TO_RUN or 'ALL'}\")\n",
    "print(f\"  Models: {MODELS_TO_RUN or 'ALL ENABLED'}\")\n",
    "print(f\"  Ensemble: {ENABLE_ENSEMBLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS & SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, TweedieRegressor\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss, log_loss,\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    roc_curve, precision_recall_curve, f1_score,\n",
    "    precision_score, recall_score\n",
    ")\n",
    "\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 120)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"✓ Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Configuration Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded models config\n",
      "✓ Loaded feature_selection config\n",
      "✓ Loaded evaluation config\n",
      "✓ Loaded cohorts config\n",
      "\n",
      "✓ All configurations loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_yaml_config(filepath: str) -> dict:\n",
    "    \"\"\"Load YAML configuration file.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# Load all configuration files\n",
    "config_files = {\n",
    "    'models': f'{CONFIG_DIR}/models.yaml',\n",
    "    'feature_selection': f'{CONFIG_DIR}/feature_selection.yaml',\n",
    "    'evaluation': f'{CONFIG_DIR}/evaluation.yaml',\n",
    "    'cohorts': f'{CONFIG_DIR}/cohorts.yaml'\n",
    "}\n",
    "\n",
    "configs = {}\n",
    "for name, filepath in config_files.items():\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"⚠️  Config file not found: {filepath}\")\n",
    "        print(f\"   Using default settings for {name}\")\n",
    "        configs[name] = {}\n",
    "    else:\n",
    "        configs[name] = load_yaml_config(filepath)\n",
    "        print(f\"✓ Loaded {name} config\")\n",
    "\n",
    "# Extract for convenience\n",
    "MODEL_CONFIG = configs['models']\n",
    "FEATURE_CONFIG = configs['feature_selection']\n",
    "EVAL_CONFIG = configs['evaluation']\n",
    "COHORT_CONFIG = configs['cohorts']\n",
    "\n",
    "print(\"\\n✓ All configurations loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Data Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data preparation functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA PREPARATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_data(train_path: str, oot_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load training and OOT feature datasets.\"\"\"\n",
    "    train = pd.read_csv(train_path)\n",
    "    oot = pd.read_csv(oot_path)\n",
    "    return train, oot\n",
    "\n",
    "\n",
    "def apply_cohort_filter(df: pd.DataFrame, cohort_name: str, \n",
    "                       cohort_config: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply cohort filters from configuration.\n",
    "    \n",
    "    Cohort config format:\n",
    "    {\n",
    "        'description': '...',\n",
    "        'filters': {\n",
    "            'column_name': {\n",
    "                'operator': '==', '>', '<', '>=', '<=', '!=', 'not_null', 'between'\n",
    "                'value': <value> or [low, high] for between\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    if cohort_name not in cohort_config['cohorts']:\n",
    "        raise ValueError(f\"Cohort '{cohort_name}' not found in config\")\n",
    "    \n",
    "    cohort_def = cohort_config['cohorts'][cohort_name]\n",
    "    filters = cohort_def.get('filters', {})\n",
    "    \n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    \n",
    "    for col, spec in filters.items():\n",
    "        if col not in df.columns:\n",
    "            print(f\"⚠️  Column '{col}' not found, skipping filter\")\n",
    "            continue\n",
    "        \n",
    "        col_data = df[col]\n",
    "        operator = spec['operator']\n",
    "        \n",
    "        if operator == 'not_null':\n",
    "            mask &= col_data.notnull()\n",
    "        elif operator == 'is_null':\n",
    "            mask &= col_data.isnull()\n",
    "        elif operator == 'between':\n",
    "            value = spec['value']\n",
    "            mask &= (col_data >= value[0]) & (col_data <= value[1])\n",
    "        elif operator == '==':\n",
    "            mask &= col_data == spec['value']\n",
    "        elif operator == '!=':\n",
    "            mask &= col_data != spec['value']\n",
    "        elif operator == '>':\n",
    "            mask &= col_data > spec['value']\n",
    "        elif operator == '>=':\n",
    "            mask &= col_data >= spec['value']\n",
    "        elif operator == '<':\n",
    "            mask &= col_data < spec['value']\n",
    "        elif operator == '<=':\n",
    "            mask &= col_data <= spec['value']\n",
    "    \n",
    "    filtered = df[mask].copy()\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(f\"  Cohort '{cohort_name}': {len(filtered):,} donors \"\n",
    "              f\"({len(filtered)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "\n",
    "def split_features_labels(df: pd.DataFrame, label: str, \n",
    "                         id_col: str = 'donor_id') -> Tuple[pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Split dataframe into features, labels, and IDs.\n",
    "    Automatically excludes:\n",
    "    - donor_id\n",
    "    - date columns (first_donation_date, last_donation_date)\n",
    "    - ALL label_* columns (including the target label)\n",
    "    \"\"\"\n",
    "    # Get donor IDs\n",
    "    if id_col in df.columns:\n",
    "        donor_ids = df[id_col].copy()\n",
    "    else:\n",
    "        donor_ids = pd.Series(range(len(df)), name='donor_id')\n",
    "    \n",
    "    # Get label FIRST (before we drop it)\n",
    "    if label not in df.columns:\n",
    "        raise ValueError(f\"Label '{label}' not found in dataframe\")\n",
    "    y = df[label].copy()\n",
    "    \n",
    "    # Exclude columns\n",
    "    exclude_cols = [id_col]\n",
    "    \n",
    "    # Exclude date columns\n",
    "    date_cols = ['first_donation_date', 'last_donation_date']\n",
    "    exclude_cols.extend([c for c in date_cols if c in df.columns])\n",
    "    \n",
    "    # Exclude ALL label_* columns (including the target label)\n",
    "    # We already extracted y above, so we can safely drop all label columns\n",
    "    label_cols = [c for c in df.columns if c.startswith('label_')]\n",
    "    exclude_cols.extend(label_cols)  # ← THIS LINE WAS MISSING!\n",
    "\n",
    "    # Manually exclude specific features\n",
    "    try:\n",
    "        from __main__ import MANUAL_EXCLUDE_FEATURES\n",
    "        exclude_cols.extend([c for c in MANUAL_EXCLUDE_FEATURES if c in df.columns])\n",
    "    except ImportError:\n",
    "        pass  # if not defined, do nothing\n",
    "\n",
    "    # Get features\n",
    "    X = df.drop(columns=exclude_cols, errors='ignore')\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(f\"  Features: {X.shape[1]} columns\")\n",
    "        print(f\"  Labels: {y.shape[0]} samples\")\n",
    "        print(f\"  Excluded: {len(exclude_cols)} columns ({len(label_cols)} label columns)\")\n",
    "    \n",
    "    return X, y, donor_ids\n",
    "\n",
    "\n",
    "def detect_task_type(y: pd.Series) -> str:\n",
    "    \"\"\"Detect if task is classification or regression.\"\"\"\n",
    "    unique_vals = y.dropna().unique()\n",
    "    \n",
    "    # Binary classification: only 0 and 1\n",
    "    if len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1}):\n",
    "        return 'classification'\n",
    "    \n",
    "    # Regression: continuous or many unique values\n",
    "    if len(unique_vals) > 10:\n",
    "        return 'regression'\n",
    "    \n",
    "    # If unclear, check dtype\n",
    "    if pd.api.types.is_numeric_dtype(y) and not pd.api.types.is_integer_dtype(y):\n",
    "        return 'regression'\n",
    "    \n",
    "    return 'classification'\n",
    "\n",
    "\n",
    "print(\"✓ Data preparation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Feature Selection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Feature selection functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE SELECTION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def remove_correlated_features(X: pd.DataFrame, threshold: float = 0.95) -> List[str]:\n",
    "    \"\"\"\n",
    "    Remove highly correlated features.\n",
    "    Returns list of features to keep.\n",
    "    \"\"\"\n",
    "    # Only numeric features\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numeric_cols) == 0:\n",
    "        return X.columns.tolist()\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = X[numeric_cols].corr().abs()\n",
    "    \n",
    "    # Find pairs above threshold\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Find features to drop\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    \n",
    "    features_to_keep = [c for c in X.columns if c not in to_drop]\n",
    "    \n",
    "    if VERBOSE and len(to_drop) > 0:\n",
    "        print(f\"  Removed {len(to_drop)} correlated features (>{threshold})\")\n",
    "    \n",
    "    return features_to_keep\n",
    "\n",
    "\n",
    "def select_features_tree_importance(X: pd.DataFrame, y: pd.Series,\n",
    "                                   top_n: int = None,\n",
    "                                   cumulative_threshold: float = 0.95,\n",
    "                                   min_features: int = 20,\n",
    "                                   max_features: int = 100) -> Tuple[List[str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Select features using tree-based importance.\n",
    "    \n",
    "    Returns:\n",
    "        features_to_keep: List of selected feature names\n",
    "        importance_df: DataFrame with feature importance scores\n",
    "    \"\"\"\n",
    "    # Quick LightGBM model for feature selection\n",
    "    task_type = detect_task_type(y)\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        model = lgb.LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            num_leaves=15,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    else:\n",
    "        model = lgb.LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            num_leaves=15,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    \n",
    "    # Handle categorical features\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    X_clean = X.copy()\n",
    "    \n",
    "    # Simple encoding for feature selection\n",
    "    if len(categorical_cols) > 0:\n",
    "        for col in categorical_cols:\n",
    "            X_clean[col] = pd.Categorical(X_clean[col]).codes\n",
    "    \n",
    "    # Fill NaNs\n",
    "    X_clean = X_clean.fillna(-999)\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(X_clean, y)\n",
    "    \n",
    "    # Get importance\n",
    "    importance = model.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Determine features to keep\n",
    "    if top_n is not None:\n",
    "        features_to_keep = importance_df.head(top_n)['feature'].tolist()\n",
    "    else:\n",
    "        # Use cumulative importance\n",
    "        importance_df['cum_importance'] = (\n",
    "            importance_df['importance'].cumsum() / importance_df['importance'].sum()\n",
    "        )\n",
    "        \n",
    "        # Get features explaining cumulative_threshold of importance\n",
    "        n_features = (importance_df['cum_importance'] <= cumulative_threshold).sum() + 1\n",
    "        \n",
    "        # Apply min/max constraints\n",
    "        n_features = max(min_features, min(n_features, max_features))\n",
    "        \n",
    "        features_to_keep = importance_df.head(n_features)['feature'].tolist()\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(f\"  Selected {len(features_to_keep)} features via tree importance\")\n",
    "        print(f\"  Top 5 features: {', '.join(features_to_keep[:5])}\")\n",
    "    \n",
    "    return features_to_keep, importance_df\n",
    "\n",
    "\n",
    "def remove_low_variance_features(X: pd.DataFrame, threshold: float = 0.01) -> List[str]:\n",
    "    \"\"\"Remove features with variance below threshold.\"\"\"\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    variances = X[numeric_cols].var()\n",
    "    keep_cols = variances[variances >= threshold].index.tolist()\n",
    "    \n",
    "    # Keep all categorical columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    features_to_keep = keep_cols + categorical_cols\n",
    "    \n",
    "    removed = len(numeric_cols) - len(keep_cols)\n",
    "    if VERBOSE and removed > 0:\n",
    "        print(f\"  Removed {removed} low-variance features (<{threshold})\")\n",
    "    \n",
    "    return features_to_keep\n",
    "\n",
    "\n",
    "def run_feature_selection(X: pd.DataFrame, y: pd.Series, \n",
    "                         config: dict) -> Tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Run complete feature selection pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        X_selected: DataFrame with selected features\n",
    "        selection_info: Dict with selection metadata\n",
    "    \"\"\"\n",
    "    if not config.get('enabled', True):\n",
    "        return X, {'method': 'none', 'n_features': X.shape[1]}\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(f\"\\nFeature Selection Pipeline:\")\n",
    "        print(f\"  Starting features: {X.shape[1]}\")\n",
    "    \n",
    "    selection_info = {\n",
    "        'initial_features': X.shape[1],\n",
    "        'stages': []\n",
    "    }\n",
    "    \n",
    "    features_to_keep = X.columns.tolist()\n",
    "    importance_df = None\n",
    "    \n",
    "    # Stage 1: Correlation filter\n",
    "    if config.get('stages', {}).get('correlation_filter', {}).get('enabled', False):\n",
    "        threshold = config['stages']['correlation_filter']['threshold']\n",
    "        features_to_keep = remove_correlated_features(X[features_to_keep], threshold)\n",
    "        selection_info['stages'].append({\n",
    "            'name': 'correlation_filter',\n",
    "            'n_features_kept': len(features_to_keep)\n",
    "        })\n",
    "    \n",
    "    # Stage 2: Tree importance\n",
    "    if config.get('stages', {}).get('tree_importance', {}).get('enabled', True):\n",
    "        tree_config = config['stages']['tree_importance']\n",
    "        auto_config = tree_config.get('auto_select', {})\n",
    "        \n",
    "        features_to_keep, importance_df = select_features_tree_importance(\n",
    "            X[features_to_keep], y,\n",
    "            top_n=tree_config.get('top_n_features'),\n",
    "            cumulative_threshold=auto_config.get('threshold', 0.95),\n",
    "            min_features=auto_config.get('min_features', 20),\n",
    "            max_features=auto_config.get('max_features', 100)\n",
    "        )\n",
    "        selection_info['stages'].append({\n",
    "            'name': 'tree_importance',\n",
    "            'n_features_kept': len(features_to_keep)\n",
    "        })\n",
    "    \n",
    "    # Stage 3: Variance filter\n",
    "    if config.get('stages', {}).get('variance_filter', {}).get('enabled', True):\n",
    "        threshold = config['stages']['variance_filter']['threshold']\n",
    "        features_to_keep = remove_low_variance_features(X[features_to_keep], threshold)\n",
    "        selection_info['stages'].append({\n",
    "            'name': 'variance_filter',\n",
    "            'n_features_kept': len(features_to_keep)\n",
    "        })\n",
    "    \n",
    "    X_selected = X[features_to_keep].copy()\n",
    "    selection_info['final_features'] = len(features_to_keep)\n",
    "    selection_info['feature_names'] = features_to_keep\n",
    "    selection_info['importance_scores'] = importance_df\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(f\"  Final features: {X_selected.shape[1]}\")\n",
    "    \n",
    "    return X_selected, selection_info\n",
    "\n",
    "\n",
    "print(\"✓ Feature selection functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: Model Building Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model building functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL BUILDING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def identify_feature_types(X: pd.DataFrame) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Identify numeric and categorical features.\"\"\"\n",
    "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    return numeric_features, categorical_features\n",
    "\n",
    "\n",
    "def create_preprocessor(numeric_features: List[str], \n",
    "                       categorical_features: List[str]) -> ColumnTransformer:\n",
    "    \"\"\"Create sklearn preprocessing pipeline.\"\"\"\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "def get_adaptive_param_grid(model_name: str, task_type: str, \n",
    "                           label_sparsity: float, n_samples: int,\n",
    "                           config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Get hyperparameter grid based on data characteristics.\n",
    "    \n",
    "    label_sparsity: Proportion of positive class (classification) or non-zero (regression)\n",
    "    n_samples: Number of training samples\n",
    "    \"\"\"\n",
    "    task_config = config.get(task_type, {})\n",
    "    model_config = task_config.get(model_name, {})\n",
    "    cv_config = model_config.get('cv_search', {})\n",
    "    \n",
    "    if not cv_config.get('enabled', False):\n",
    "        return {}\n",
    "    \n",
    "    adaptive_rules = cv_config.get('adaptive_rules', {})\n",
    "    \n",
    "    # Determine which rule to use for LGBM\n",
    "    if 'lgbm' in model_name:\n",
    "        # Check small sample\n",
    "        small_sample_threshold = adaptive_rules.get('small_sample', {}).get('threshold', 10000)\n",
    "        \n",
    "        if n_samples < small_sample_threshold:\n",
    "            rule = adaptive_rules.get('small_sample', {})\n",
    "            if VERBOSE:\n",
    "                print(f\"    Using 'small_sample' grid (n={n_samples:,})\")\n",
    "        \n",
    "        # Classification: check sparse_label vs dense_label\n",
    "        elif task_type == 'classification':\n",
    "            sparse_threshold = adaptive_rules.get('sparse_label', {}).get('threshold', 0.05)\n",
    "            dense_threshold = adaptive_rules.get('dense_label', {}).get('threshold', 0.05)\n",
    "            \n",
    "            if label_sparsity < sparse_threshold:\n",
    "                rule = adaptive_rules.get('sparse_label', {})\n",
    "                if VERBOSE:\n",
    "                    print(f\"    Using 'sparse_label' grid (prevalence={label_sparsity:.2%})\")\n",
    "            elif label_sparsity >= dense_threshold:\n",
    "                rule = adaptive_rules.get('dense_label', {})\n",
    "                if VERBOSE:\n",
    "                    print(f\"    Using 'dense_label' grid (prevalence={label_sparsity:.2%})\")\n",
    "            else:\n",
    "                rule = adaptive_rules.get('default', {})\n",
    "                if VERBOSE:\n",
    "                    print(f\"    Using 'default' grid\")\n",
    "        \n",
    "        # Regression: check sparse_outcomes\n",
    "        else:  # task_type == 'regression'\n",
    "            sparse_threshold = adaptive_rules.get('sparse_outcomes', {}).get('threshold', 0.10)\n",
    "            \n",
    "            if label_sparsity < sparse_threshold:\n",
    "                rule = adaptive_rules.get('sparse_outcomes', {})\n",
    "                if VERBOSE:\n",
    "                    print(f\"    Using 'sparse_outcomes' grid (non-zero={label_sparsity:.2%})\")\n",
    "            else:\n",
    "                rule = adaptive_rules.get('default', {})\n",
    "                if VERBOSE:\n",
    "                    print(f\"    Using 'default' grid\")\n",
    "        \n",
    "        param_grid = rule.get('param_grid', {})\n",
    "    else:\n",
    "        # Non-LGBM models use standard param_grid\n",
    "        param_grid = cv_config.get('param_grid', {})\n",
    "    \n",
    "    return param_grid\n",
    "\n",
    "\n",
    "def build_model_pipeline(model_name: str, task_type: str, \n",
    "                        model_params: dict, X: pd.DataFrame) -> Pipeline:\n",
    "    \"\"\"Build sklearn pipeline with preprocessing + model.\"\"\"\n",
    "    numeric_features, categorical_features = identify_feature_types(X)\n",
    "    preprocessor = create_preprocessor(numeric_features, categorical_features)\n",
    "    \n",
    "    # Create model\n",
    "    if task_type == 'classification':\n",
    "        if model_name == 'logreg':\n",
    "            model = LogisticRegression(**model_params)\n",
    "        elif model_name == 'lgbm':\n",
    "            model = lgb.LGBMClassifier(**model_params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown classifier: {model_name}\")\n",
    "    else:  # regression\n",
    "        if model_name == 'ridge':\n",
    "            model = Ridge(**model_params)\n",
    "        elif model_name == 'lgbm_regressor':\n",
    "            model = lgb.LGBMRegressor(**model_params)\n",
    "        elif model_name == 'tweedie':\n",
    "            model = TweedieRegressor(**model_params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown regressor: {model_name}\")\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def run_cv_grid_search(X: pd.DataFrame, y: pd.Series, \n",
    "                      model_name: str, task_type: str,\n",
    "                      config: dict) -> Tuple[Pipeline, dict]:\n",
    "    \"\"\"\n",
    "    Run CV grid search with adaptive parameter grids.\n",
    "    \n",
    "    Returns:\n",
    "        best_pipeline: Fitted pipeline with best params\n",
    "        cv_results: Dictionary with CV metadata\n",
    "    \"\"\"\n",
    "    task_config = config.get(task_type, {})\n",
    "    model_config = task_config.get(model_name, {})\n",
    "    \n",
    "    # Get base parameters\n",
    "    base_params = model_config.get('base_params', {})\n",
    "    \n",
    "    # Get CV configuration\n",
    "    cv_config = model_config.get('cv_search', {})\n",
    "    \n",
    "    if not cv_config.get('enabled', False):\n",
    "        # No CV, just train with base params\n",
    "        pipeline = build_model_pipeline(model_name, task_type, base_params, X)\n",
    "        pipeline.fit(X, y)\n",
    "        return pipeline, {'cv_enabled': False, 'best_params': base_params}\n",
    "    \n",
    "    # Calculate label characteristics\n",
    "    if task_type == 'classification':\n",
    "        label_sparsity = y.mean()\n",
    "    else:\n",
    "        label_sparsity = (y != 0).mean()\n",
    "    \n",
    "    # Get adaptive param grid\n",
    "    param_grid = get_adaptive_param_grid(\n",
    "        model_name, task_type, label_sparsity, len(X), config\n",
    "    )\n",
    "    \n",
    "    if not param_grid:\n",
    "        # No grid specified, use base params\n",
    "        pipeline = build_model_pipeline(model_name, task_type, base_params, X)\n",
    "        pipeline.fit(X, y)\n",
    "        return pipeline, {'cv_enabled': False, 'best_params': base_params}\n",
    "    \n",
    "    # Prepend 'model__' to param names for pipeline\n",
    "    param_grid_pipeline = {f'model__{k}': v for k, v in param_grid.items()}\n",
    "    \n",
    "    # Build base pipeline\n",
    "    pipeline = build_model_pipeline(model_name, task_type, base_params, X)\n",
    "    \n",
    "    # Run grid search\n",
    "    cv_folds = cv_config.get('cv_folds', 5)\n",
    "    scoring = cv_config.get('scoring', 'roc_auc' if task_type == 'classification' else 'neg_mean_squared_error')\n",
    "    \n",
    "    # Calculate total possible combinations\n",
    "    n_combinations = int(np.prod([len(v) for v in param_grid.values()]))\n",
    "    \n",
    "    # Get N_ITER from config (if specified)\n",
    "    n_iter = cv_config.get('n_iter', None)\n",
    "    \n",
    "    # Use RandomizedSearchCV if n_iter specified and less than total combinations\n",
    "    if n_iter and n_iter < n_combinations:\n",
    "        search = RandomizedSearchCV(\n",
    "            pipeline,\n",
    "            param_grid_pipeline,\n",
    "            n_iter=n_iter,\n",
    "            cv=KFold(n_splits=cv_folds, shuffle=True, random_state=42),\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            verbose=0,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        if VERBOSE:\n",
    "            print(f\"    Running {cv_folds}-fold CV over {n_iter} random combinations (from {n_combinations} possible)...\")\n",
    "    else:\n",
    "        search = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid_pipeline,\n",
    "            cv=KFold(n_splits=cv_folds, shuffle=True, random_state=42),\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        if VERBOSE:\n",
    "            print(f\"    Running {cv_folds}-fold CV over {n_combinations} parameter combinations...\")\n",
    "    \n",
    "    search.fit(X, y)\n",
    "    \n",
    "    best_params_full = search.best_params_\n",
    "    # Remove 'model__' prefix\n",
    "    best_params = {k.replace('model__', ''): v for k, v in best_params_full.items()}\n",
    "    \n",
    "    cv_results = {\n",
    "        'cv_enabled': True,\n",
    "        'best_params': best_params,\n",
    "        'best_score': search.best_score_,\n",
    "        'cv_folds': cv_folds,\n",
    "        'n_combinations': n_iter if n_iter else n_combinations\n",
    "    }\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(f\"    Best CV score: {search.best_score_:.4f}\")\n",
    "        print(f\"    Best params: {best_params}\")\n",
    "    \n",
    "    return search.best_estimator_, cv_results\n",
    "\n",
    "\n",
    "def calibrate_model(pipeline: Pipeline, X: pd.DataFrame, y: pd.Series,\n",
    "                   config: dict, model_name: str, task_type: str) -> Pipeline:\n",
    "    \"\"\"Apply calibration to classification models.\"\"\"\n",
    "    if task_type != 'classification':\n",
    "        return pipeline\n",
    "    \n",
    "    task_config = config.get(task_type, {})\n",
    "    model_config = task_config.get(model_name, {})\n",
    "    calib_config = model_config.get('calibration', {})\n",
    "    \n",
    "    if not calib_config.get('enabled', False):\n",
    "        return pipeline\n",
    "    \n",
    "    method = calib_config.get('method', 'isotonic')\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(f\"    Applying {method} calibration...\")\n",
    "    \n",
    "    calibrated = CalibratedClassifierCV(\n",
    "        pipeline,\n",
    "        method=method,\n",
    "        cv='prefit',\n",
    "        ensemble=False\n",
    "    )\n",
    "    \n",
    "    calibrated.fit(X, y)\n",
    "    \n",
    "    return calibrated\n",
    "\n",
    "\n",
    "print(\"✓ Model building functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 6: Ensemble Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ensemble functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENSEMBLE FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "class WeightedEnsemble:\n",
    "    \"\"\"Sklearn-compatible weighted ensemble.\"\"\"\n",
    "    \n",
    "    def __init__(self, models: dict, weights: dict, task_type: str):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "        self.task_type = task_type\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Weighted average of probabilities (classification only).\"\"\"\n",
    "        if self.task_type != 'classification':\n",
    "            raise ValueError(\"predict_proba only for classification\")\n",
    "        \n",
    "        predictions = []\n",
    "        for name, model in self.models.items():\n",
    "            pred = model.predict_proba(X)\n",
    "            predictions.append(pred * self.weights[name])\n",
    "        \n",
    "        return np.sum(predictions, axis=0)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Weighted average prediction.\"\"\"\n",
    "        if self.task_type == 'classification':\n",
    "            proba = self.predict_proba(X)\n",
    "            return (proba[:, 1] > 0.5).astype(int)\n",
    "        else:\n",
    "            predictions = []\n",
    "            for name, model in self.models.items():\n",
    "                pred = model.predict(X)\n",
    "                predictions.append(pred * self.weights[name])\n",
    "            return np.sum(predictions, axis=0)\n",
    "\n",
    "\n",
    "def should_create_ensemble(model_performances: dict, config: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Decide if ensemble is worth creating based on performance similarity.\n",
    "    Only create if models are \"close\" in performance.\n",
    "    \"\"\"\n",
    "    ensemble_config = config.get('ensemble', {})\n",
    "    \n",
    "    if not ensemble_config.get('enabled', False):\n",
    "        return False\n",
    "    \n",
    "    similarity_threshold = ensemble_config.get('similarity_threshold', 0.02)\n",
    "    \n",
    "    # Get performance metric for each model\n",
    "    scores = [perf['primary_metric'] for perf in model_performances.values()]\n",
    "    \n",
    "    if len(scores) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Check if models are similar in performance\n",
    "    score_range = max(scores) - min(scores)\n",
    "    \n",
    "    if score_range > similarity_threshold:\n",
    "        if VERBOSE:\n",
    "            print(f\"  Ensemble skipped: performance gap ({score_range:.4f}) \"\n",
    "                  f\"exceeds threshold ({similarity_threshold})\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def calculate_ensemble_weights(model_performances: dict, \n",
    "                              strategy: str = 'performance') -> dict:\n",
    "    \"\"\"\n",
    "    Calculate ensemble weights based on strategy.\n",
    "    \n",
    "    Strategies:\n",
    "    - 'equal': All models weighted equally\n",
    "    - 'performance': Weight by relative performance\n",
    "    \"\"\"\n",
    "    if strategy == 'equal':\n",
    "        n_models = len(model_performances)\n",
    "        return {name: 1.0 / n_models for name in model_performances.keys()}\n",
    "    \n",
    "    elif strategy == 'performance':\n",
    "        # Weight by performance score\n",
    "        scores = {name: perf['primary_metric'] \n",
    "                 for name, perf in model_performances.items()}\n",
    "        \n",
    "        # Normalize to sum to 1\n",
    "        total_score = sum(scores.values())\n",
    "        weights = {name: score / total_score \n",
    "                  for name, score in scores.items()}\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown weighting strategy: {strategy}\")\n",
    "\n",
    "\n",
    "def create_ensemble(models: dict, model_performances: dict,\n",
    "                   config: dict, task_type: str) -> Optional[WeightedEnsemble]:\n",
    "    \"\"\"\n",
    "    Create weighted ensemble if appropriate.\n",
    "    \n",
    "    Returns:\n",
    "        WeightedEnsemble or None if ensemble not created\n",
    "    \"\"\"\n",
    "    if not should_create_ensemble(model_performances, config):\n",
    "        return None\n",
    "    \n",
    "    ensemble_config = config.get('ensemble', {})\n",
    "    weighting_config = ensemble_config.get('weighting_strategy', {})\n",
    "    strategy = weighting_config.get('method', 'performance')\n",
    "    \n",
    "    weights = calculate_ensemble_weights(model_performances, strategy)\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(f\"\\n  Creating ensemble with {strategy} weighting:\")\n",
    "        for name, weight in weights.items():\n",
    "            print(f\"    {name}: {weight:.3f}\")\n",
    "    \n",
    "    ensemble = WeightedEnsemble(models, weights, task_type)\n",
    "    \n",
    "    return ensemble\n",
    "\n",
    "\n",
    "print(\"✓ Ensemble functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 7: Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_classification(y_true: np.ndarray, y_pred_proba: np.ndarray,\n",
    "                           config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate all classification metrics.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - core metrics (AUC, PR-AUC, Brier, etc.)\n",
    "    - lift table\n",
    "    - lift at K values\n",
    "    \"\"\"\n",
    "    eval_config = config.get('classification', {})\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Core metrics\n",
    "    metrics['auc'] = roc_auc_score(y_true, y_pred_proba)\n",
    "    metrics['pr_auc'] = average_precision_score(y_true, y_pred_proba)\n",
    "    metrics['brier_score'] = brier_score_loss(y_true, y_pred_proba)\n",
    "    metrics['log_loss'] = log_loss(y_true, y_pred_proba)\n",
    "    metrics['baseline_rate'] = y_true.mean()\n",
    "    \n",
    "    # Lift table\n",
    "    lift_config = eval_config.get('lift_table', {})\n",
    "    n_quantiles = lift_config.get('n_quantiles', 20)\n",
    "    lift_at_k = lift_config.get('lift_at_k', [0.01, 0.05, 0.10])\n",
    "    \n",
    "    df_lift = pd.DataFrame({\n",
    "        'y_true': y_true,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    })\n",
    "    \n",
    "    df_lift = df_lift.sort_values('y_pred_proba', ascending=False).reset_index(drop=True)\n",
    "    df_lift['quantile'] = pd.qcut(df_lift.index, n_quantiles, labels=False, duplicates='drop') + 1\n",
    "    \n",
    "    lift_table = df_lift.groupby('quantile').agg(\n",
    "        n=('y_true', 'count'),\n",
    "        n_events=('y_true', 'sum'),\n",
    "        avg_score=('y_pred_proba', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    baseline = y_true.mean()\n",
    "    lift_table['event_rate'] = lift_table['n_events'] / lift_table['n']\n",
    "    lift_table['lift'] = lift_table['event_rate'] / baseline\n",
    "    lift_table['cum_n'] = lift_table['n'].cumsum()\n",
    "    lift_table['cum_n_events'] = lift_table['n_events'].cumsum()\n",
    "    lift_table['cum_event_rate'] = lift_table['cum_n_events'] / lift_table['cum_n']\n",
    "    lift_table['cum_lift'] = lift_table['cum_event_rate'] / baseline\n",
    "    lift_table['cum_pct_captured'] = lift_table['cum_n_events'] / y_true.sum()\n",
    "    \n",
    "    metrics['lift_table'] = lift_table\n",
    "    \n",
    "    # Lift at K\n",
    "    for k in lift_at_k:\n",
    "        n_k = max(1, int(len(df_lift) * k))\n",
    "        top_k_events = df_lift.head(n_k)['y_true'].sum()\n",
    "        expected_k_events = baseline * n_k\n",
    "        lift_k = top_k_events / expected_k_events if expected_k_events > 0 else 0\n",
    "        metrics[f'lift_at_{k}'] = lift_k\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_regression(y_true: np.ndarray, y_pred: np.ndarray,\n",
    "                       config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate all regression metrics.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['rmse'] = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    metrics['mae'] = mean_absolute_error(y_true, y_pred)\n",
    "    metrics['r2'] = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # MAPE (avoid division by zero)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() > 0:\n",
    "        metrics['mape'] = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "    else:\n",
    "        metrics['mape'] = np.nan\n",
    "    \n",
    "    # Baseline (predict mean)\n",
    "    baseline_pred = np.full_like(y_true, y_true.mean())\n",
    "    metrics['baseline_rmse'] = np.sqrt(mean_squared_error(y_true, baseline_pred))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def select_best_model(candidate_results: dict, task_type: str, config: dict) -> str:\n",
    "    \"\"\"\n",
    "    Select best model based on configuration criteria.\n",
    "    \n",
    "    Returns:\n",
    "        Name of best model\n",
    "    \"\"\"\n",
    "    selection_config = config.get('best_model_selection', {})\n",
    "    \n",
    "    # Check for manual override\n",
    "    manual = selection_config.get('manual_selection')\n",
    "    if manual and manual in candidate_results:\n",
    "        if VERBOSE:\n",
    "            print(f\"  Manual selection: {manual}\")\n",
    "        return manual\n",
    "    \n",
    "    # Get primary metric\n",
    "    primary_metrics = selection_config.get('primary_metric', {})\n",
    "    primary_metric = primary_metrics.get(task_type, 'oot_auc' if task_type == 'classification' else 'oot_rmse')\n",
    "    \n",
    "    # Check for overfitting penalty\n",
    "    overfit_config = selection_config.get('overfit_penalty', {})\n",
    "    penalize_overfit = overfit_config.get('enabled', True)\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        max_gap = overfit_config.get('max_gap', {}).get('classification', 0.05)\n",
    "        \n",
    "        # Find model with best OOT performance and acceptable train/OOT gap\n",
    "        valid_models = {}\n",
    "        for name, results in candidate_results.items():\n",
    "            train_metric = results['metrics']['train'][primary_metric.replace('oot_', '')]\n",
    "            oot_metric = results['metrics']['oot'][primary_metric.replace('oot_', '')]\n",
    "            gap = train_metric - oot_metric\n",
    "            \n",
    "            if not penalize_overfit or gap <= max_gap:\n",
    "                valid_models[name] = oot_metric\n",
    "        \n",
    "        if not valid_models:\n",
    "            if VERBOSE:\n",
    "                print(f\"  ⚠️  All models exceed overfit threshold, selecting best OOT anyway\")\n",
    "            valid_models = {name: results['metrics']['oot'][primary_metric.replace('oot_', '')]\n",
    "                          for name, results in candidate_results.items()}\n",
    "        \n",
    "        # Maximize AUC\n",
    "        best_model = max(valid_models, key=valid_models.get)\n",
    "        \n",
    "    else:  # regression\n",
    "        max_gap_ratio = overfit_config.get('max_gap', {}).get('regression', 0.20)\n",
    "        \n",
    "        valid_models = {}\n",
    "        for name, results in candidate_results.items():\n",
    "            train_metric = results['metrics']['train']['rmse']\n",
    "            oot_metric = results['metrics']['oot']['rmse']\n",
    "            gap_ratio = (oot_metric - train_metric) / train_metric if train_metric > 0 else 999\n",
    "            \n",
    "            if not penalize_overfit or gap_ratio <= max_gap_ratio:\n",
    "                valid_models[name] = oot_metric\n",
    "        \n",
    "        if not valid_models:\n",
    "            if VERBOSE:\n",
    "                print(f\"  ⚠️  All models exceed overfit threshold, selecting best OOT anyway\")\n",
    "            valid_models = {name: results['metrics']['oot']['rmse']\n",
    "                          for name, results in candidate_results.items()}\n",
    "        \n",
    "        # Minimize RMSE\n",
    "        best_model = min(valid_models, key=valid_models.get)\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(f\"  Best model: {best_model} ({primary_metric}={valid_models[best_model]:.4f})\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "\n",
    "print(\"✓ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 8: Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Threshold optimization functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# THRESHOLD OPTIMIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def optimize_thresholds(y_true: np.ndarray, y_pred_proba: np.ndarray,\n",
    "                       config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Find optimal thresholds for different business objectives.\n",
    "    \n",
    "    Returns dict with threshold for each objective.\n",
    "    \"\"\"\n",
    "    threshold_config = config.get('threshold_optimization', {})\n",
    "    \n",
    "    if not threshold_config.get('enabled', False):\n",
    "        return {}\n",
    "    \n",
    "    search_config = threshold_config.get('search', {})\n",
    "    n_thresholds = search_config.get('n_thresholds', 100)\n",
    "    min_threshold = search_config.get('min_threshold', 0.01)\n",
    "    max_threshold = search_config.get('max_threshold', 0.99)\n",
    "    \n",
    "    thresholds_to_test = np.linspace(min_threshold, max_threshold, n_thresholds)\n",
    "    \n",
    "    results = {}\n",
    "    objectives_config = threshold_config.get('objectives', {})\n",
    "    \n",
    "    # Calculate metrics at each threshold\n",
    "    threshold_metrics = []\n",
    "    for thresh in thresholds_to_test:\n",
    "        y_pred = (y_pred_proba >= thresh).astype(int)\n",
    "        \n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        \n",
    "        threshold_metrics.append({\n",
    "            'threshold': thresh,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        })\n",
    "    \n",
    "    df_metrics = pd.DataFrame(threshold_metrics)\n",
    "    \n",
    "    # Find optimal threshold for each objective\n",
    "    for obj_name, obj_config in objectives_config.items():\n",
    "        metric = obj_config.get('metric')\n",
    "        optimization = obj_config.get('optimization')\n",
    "        \n",
    "        if obj_name == 'profit_optimal' and not obj_config.get('enabled', False):\n",
    "            continue\n",
    "        \n",
    "        if optimization == 'maximize':\n",
    "            if metric == 'profit':\n",
    "                cost_per_contact = obj_config.get('cost_per_contact', 2.0)\n",
    "                value_per_conversion = obj_config.get('value_per_conversion', 50.0)\n",
    "                \n",
    "                profits = []\n",
    "                for _, row in df_metrics.iterrows():\n",
    "                    thresh = row['threshold']\n",
    "                    y_pred = (y_pred_proba >= thresh).astype(int)\n",
    "                    \n",
    "                    n_contacts = y_pred.sum()\n",
    "                    n_conversions = (y_true & y_pred).sum()\n",
    "                    \n",
    "                    profit = (n_conversions * value_per_conversion) - (n_contacts * cost_per_contact)\n",
    "                    profits.append(profit)\n",
    "                \n",
    "                df_metrics['profit'] = profits\n",
    "                best_idx = df_metrics['profit'].idxmax()\n",
    "                results[obj_name] = {\n",
    "                    'threshold': df_metrics.loc[best_idx, 'threshold'],\n",
    "                    'profit': df_metrics.loc[best_idx, 'profit']\n",
    "                }\n",
    "            else:\n",
    "                best_idx = df_metrics[metric].idxmax()\n",
    "                results[obj_name] = {\n",
    "                    'threshold': df_metrics.loc[best_idx, 'threshold'],\n",
    "                    metric: df_metrics.loc[best_idx, metric]\n",
    "                }\n",
    "        \n",
    "        elif optimization == 'reach_target':\n",
    "            target_value = obj_config.get('target_value')\n",
    "            # Find threshold closest to target\n",
    "            df_metrics['distance'] = np.abs(df_metrics[metric] - target_value)\n",
    "            best_idx = df_metrics['distance'].idxmin()\n",
    "            results[obj_name] = {\n",
    "                'threshold': df_metrics.loc[best_idx, 'threshold'],\n",
    "                metric: df_metrics.loc[best_idx, metric],\n",
    "                'target': target_value\n",
    "            }\n",
    "    \n",
    "    # Store full threshold curve for plotting\n",
    "    results['_threshold_curve'] = df_metrics\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(f\"\\n  Threshold Optimization:\")\n",
    "        for obj_name, obj_result in results.items():\n",
    "            if obj_name != '_threshold_curve':\n",
    "                print(f\"    {obj_name}: threshold={obj_result['threshold']:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"✓ Threshold optimization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 9: SHAP / Explainability Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SHAP functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SHAP / EXPLAINABILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def generate_shap_values(base_model, X, eval_config=None):\n",
    "    \"\"\"\n",
    "    Generate SHAP values for tree-based models only.\n",
    "    For linear models (LogisticRegression), we skip SHAP and rely on\n",
    "    coefficient / odds-ratio tables instead.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_model : estimator or sklearn.Pipeline\n",
    "        Fitted model or pipeline. If a Pipeline, we expect a 'preprocessor'\n",
    "        and 'model' step; otherwise we treat it as a plain estimator.\n",
    "    X : pd.DataFrame\n",
    "        Original feature data (already feature-selected in your pipeline).\n",
    "    eval_config : dict, optional\n",
    "        Can carry SHAP-related settings, e.g.:\n",
    "          - 'shap_sample_size': int or None\n",
    "          - 'random_state': int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    shap_values : array-like or None\n",
    "        SHAP values for the sampled data, or None if SHAP is skipped/failed.\n",
    "    \"\"\"\n",
    "    # --- Pull options from eval_config ---\n",
    "    sample_size = None\n",
    "    random_state = 42\n",
    "    if isinstance(eval_config, dict):\n",
    "        sample_size = eval_config.get(\"shap_sample_size\", sample_size)\n",
    "        random_state = eval_config.get(\"random_state\", random_state)\n",
    "\n",
    "    # --- Subsample X if requested ---\n",
    "    if sample_size is not None and len(X) > sample_size:\n",
    "        X_sample = X.sample(n=sample_size, random_state=random_state)\n",
    "    else:\n",
    "        X_sample = X\n",
    "\n",
    "    # --- Transform with preprocessor if present (Pipeline case) ---\n",
    "    preprocessor = None\n",
    "    if hasattr(base_model, \"named_steps\"):\n",
    "        preprocessor = base_model.named_steps.get(\"preprocessor\", None)\n",
    "\n",
    "    if preprocessor is not None:\n",
    "        X_transformed = preprocessor.transform(X_sample)\n",
    "    else:\n",
    "        # assume X is already numeric or compatible with the model\n",
    "        X_transformed = X_sample.values\n",
    "\n",
    "    # --- Get underlying estimator ---\n",
    "    if hasattr(base_model, \"named_steps\") and \"model\" in base_model.named_steps:\n",
    "        actual_model = base_model.named_steps[\"model\"]\n",
    "    else:\n",
    "        actual_model = base_model\n",
    "\n",
    "    # --- If it's logistic, skip SHAP (we'll use coeff table instead) ---\n",
    "    if isinstance(actual_model, LogisticRegression):\n",
    "        print(\"Skipping SHAP for LogisticRegression – using coefficient/odds table instead.\")\n",
    "        return None\n",
    "\n",
    "    # --- Otherwise, run SHAP (tree or other) ---\n",
    "    try:\n",
    "        explainer = shap.Explainer(actual_model, X_transformed)\n",
    "\n",
    "        # For TreeExplainer, disable strict additivity check\n",
    "        if isinstance(explainer, shap.explainers._tree.TreeExplainer):\n",
    "            shap_values = explainer(X_transformed, check_additivity=False)\n",
    "        else:\n",
    "            shap_values = explainer(X_transformed)\n",
    "\n",
    "        return shap_values\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️  SHAP generation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def summarize_shap_direction(shap_dict: dict, top_n: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarize SHAP directionality (positive vs negative impact).\n",
    "    \n",
    "    Returns DataFrame with:\n",
    "    - feature: Feature name\n",
    "    - mean_abs_shap: Mean absolute SHAP value\n",
    "    - corr_feature_shap: Correlation between feature value and SHAP value\n",
    "    - direction: Interpretation of direction\n",
    "    \"\"\"\n",
    "    if shap_dict is None:\n",
    "        return None\n",
    "    \n",
    "    shap_values = shap_dict['values']\n",
    "    X_data = shap_dict['data']\n",
    "    feature_names = shap_dict['feature_names']\n",
    "    \n",
    "    mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
    "    order = np.argsort(mean_abs_shap)[::-1][:top_n]\n",
    "    \n",
    "    rows = []\n",
    "    for idx in order:\n",
    "        fname = feature_names[idx]\n",
    "        vals = X_data[:, idx]\n",
    "        shap_col = shap_values[:, idx]\n",
    "        \n",
    "        # Drop NaNs if any\n",
    "        mask = ~np.isnan(vals) & ~np.isnan(shap_col)\n",
    "        vals = vals[mask]\n",
    "        shap_col = shap_col[mask]\n",
    "        \n",
    "        if len(vals) < 10:\n",
    "            corr = np.nan\n",
    "            sign = \"unknown\"\n",
    "        else:\n",
    "            corr = np.corrcoef(vals, shap_col)[0, 1]\n",
    "            if corr > 0.05:\n",
    "                sign = \"higher value → higher prediction\"\n",
    "            elif corr < -0.05:\n",
    "                sign = \"higher value → lower prediction\"\n",
    "            else:\n",
    "                sign = \"mixed / weak\"\n",
    "        \n",
    "        rows.append({\n",
    "            \"feature\": fname,\n",
    "            \"mean_abs_shap\": mean_abs_shap[idx],\n",
    "            \"corr_feature_shap\": corr,\n",
    "            \"direction\": sign\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def build_logistic_coeff_table(\n",
    "    base_model,\n",
    "    feature_names=None,\n",
    "    top_n=None,\n",
    "    round_digits=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build an interpretability table for a logistic regression model:\n",
    "    - coefficient (log-odds)\n",
    "    - odds ratio (exp(coefficient))\n",
    "    - absolute coefficient for ranking\n",
    "    - plain-English explanation of odds ratio\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_model : sklearn.Pipeline\n",
    "        Fitted pipeline with a 'model' step that is a LogisticRegression.\n",
    "    feature_names : list of str, optional\n",
    "        Names of the features after preprocessing. If None, will try\n",
    "        base_model.named_steps['preprocessor'].get_feature_names_out().\n",
    "    top_n : int, optional\n",
    "        If provided, return only the top_n features by |coefficient|.\n",
    "    round_digits : int, optional\n",
    "        Number of decimal places for rounding numeric columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    coeff_df : pandas.DataFrame\n",
    "        Columns:\n",
    "        ['feature', 'coefficient', 'odds_ratio',\n",
    "         'abs_coefficient', 'interpretation']\n",
    "        Sorted by |coefficient| descending.\n",
    "    \"\"\"\n",
    "    # 1. Get the underlying model\n",
    "    model = base_model.named_steps.get(\"model\", None)\n",
    "    if model is None:\n",
    "        raise ValueError(\"Pipeline must have a 'model' step.\")\n",
    "    if not isinstance(model, LogisticRegression):\n",
    "        raise ValueError(\n",
    "            f\"'model' step must be LogisticRegression, got {type(model)} instead.\"\n",
    "        )\n",
    "\n",
    "    # 2. Resolve feature names\n",
    "    if feature_names is None:\n",
    "        preprocessor = base_model.named_steps.get(\"preprocessor\", None)\n",
    "        if preprocessor is not None and hasattr(preprocessor, \"get_feature_names_out\"):\n",
    "            feature_names = preprocessor.get_feature_names_out()\n",
    "        else:\n",
    "            n_features = model.coef_.shape[1]\n",
    "            feature_names = [f\"feature_{i}\" for i in range(n_features)]\n",
    "\n",
    "    # 3. Extract coefficients (binary classification: (1, n_features))\n",
    "    coef = model.coef_.ravel()\n",
    "    odds_ratio = np.exp(coef)\n",
    "\n",
    "    coeff_df = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": feature_names,\n",
    "            \"coefficient\": coef,\n",
    "            \"odds_ratio\": odds_ratio,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 4. Add abs(coef) for ranking\n",
    "    coeff_df[\"abs_coefficient\"] = coeff_df[\"coefficient\"].abs()\n",
    "\n",
    "    # 5. Sort by importance\n",
    "    coeff_df = coeff_df.sort_values(\"abs_coefficient\", ascending=False)\n",
    "\n",
    "    # 6. Keep only top_n if requested\n",
    "    if top_n is not None:\n",
    "        coeff_df = coeff_df.head(top_n)\n",
    "\n",
    "    # 7. Round numeric columns for readability\n",
    "    numeric_cols = [\"coefficient\", \"odds_ratio\", \"abs_coefficient\"]\n",
    "    coeff_df[numeric_cols] = coeff_df[numeric_cols].round(round_digits)\n",
    "\n",
    "    # 8. Build plain-English interpretation column\n",
    "    interpretations = []\n",
    "    for _, row in coeff_df.iterrows():\n",
    "        feat = row[\"feature\"]\n",
    "        or_val = row[\"odds_ratio\"]\n",
    "\n",
    "        # percent change in odds for a 1-unit increase\n",
    "        pct_change = (or_val - 1.0) * 100.0\n",
    "        pct_change_rounded = round(pct_change, 1)\n",
    "\n",
    "        if abs(pct_change_rounded) < 0.5:\n",
    "            text = (\n",
    "                f\"A 1-unit increase in '{feat}' is associated with \"\n",
    "                f\"almost no change in the odds of the positive outcome.\"\n",
    "            )\n",
    "        elif pct_change_rounded > 0:\n",
    "            text = (\n",
    "                f\"A 1-unit increase in '{feat}' is associated with \"\n",
    "                f\"approximately {pct_change_rounded}% higher odds of the \"\n",
    "                f\"positive outcome\"\n",
    "            )\n",
    "        else:\n",
    "            text = (\n",
    "                f\"A 1-unit increase in '{feat}' is associated with \"\n",
    "                f\"approximately {abs(pct_change_rounded)}% lower odds of the \"\n",
    "                f\"positive outcome\"\n",
    "            )\n",
    "\n",
    "        interpretations.append(text)\n",
    "\n",
    "    coeff_df[\"interpretation\"] = interpretations\n",
    "\n",
    "    return coeff_df\n",
    "\n",
    "\n",
    "print(\"✓ SHAP functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 10: Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Visualization functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def plot_lift_curve(lift_table: pd.DataFrame, output_path: str):\n",
    "    \"\"\"Plot lift curve from lift table.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Cumulative lift\n",
    "    ax1.plot(lift_table['quantile'], lift_table['cum_lift'], marker='o', linewidth=2)\n",
    "    ax1.axhline(y=1, color='red', linestyle='--', label='Baseline')\n",
    "    ax1.set_xlabel('Quantile')\n",
    "    ax1.set_ylabel('Cumulative Lift')\n",
    "    ax1.set_title('Cumulative Lift')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cumulative % captured\n",
    "    ax2.plot(lift_table['quantile'], lift_table['cum_pct_captured'] * 100, \n",
    "             marker='o', linewidth=2)\n",
    "    ax2.set_xlabel('Quantile')\n",
    "    ax2.set_ylabel('% Events Captured')\n",
    "    ax2.set_title('Event Capture')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_calibration_curve(y_true: np.ndarray, y_pred_proba: np.ndarray,\n",
    "                          output_path: str, n_bins: int = 10):\n",
    "    \"\"\"Plot calibration curve.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Calculate calibration\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_indices = np.digitize(y_pred_proba, bins) - 1\n",
    "    bin_indices = np.clip(bin_indices, 0, n_bins - 1)\n",
    "    \n",
    "    bin_true = np.zeros(n_bins)\n",
    "    bin_pred = np.zeros(n_bins)\n",
    "    bin_count = np.zeros(n_bins)\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        mask = bin_indices == i\n",
    "        if mask.sum() > 0:\n",
    "            bin_true[i] = y_true[mask].mean()\n",
    "            bin_pred[i] = y_pred_proba[mask].mean()\n",
    "            bin_count[i] = mask.sum()\n",
    "    \n",
    "    # Calibration curve\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "    ax1.plot(bin_pred, bin_true, marker='o', linewidth=2, label='Model')\n",
    "    ax1.set_xlabel('Predicted Probability')\n",
    "    ax1.set_ylabel('Actual Probability')\n",
    "    ax1.set_title('Calibration Curve')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution of predictions\n",
    "    ax2.hist(y_pred_proba, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_xlabel('Predicted Probability')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_title('Distribution of Predictions')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_shap_summary(shap_dict: dict, output_path: str, top_n: int = 20):\n",
    "    \"\"\"Plot SHAP summary.\"\"\"\n",
    "    if shap_dict is None:\n",
    "        print(\"  ⚠️  No SHAP data to plot\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        shap_values = shap_dict['values']\n",
    "        X_data = shap_dict['data']\n",
    "        feature_names = shap_dict['feature_names']\n",
    "        \n",
    "        plt.figure(figsize=(10, max(6, top_n * 0.3)))\n",
    "        shap.summary_plot(\n",
    "            shap_values,\n",
    "            X_data,\n",
    "            feature_names=feature_names,\n",
    "            max_display=top_n,\n",
    "            show=False\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️  SHAP plot failed: {e}\")\n",
    "\n",
    "\n",
    "def plot_threshold_analysis(threshold_results: dict, output_path: str):\n",
    "    \"\"\"Plot precision/recall/F1 vs threshold.\"\"\"\n",
    "    if '_threshold_curve' not in threshold_results:\n",
    "        return\n",
    "    \n",
    "    df = threshold_results['_threshold_curve']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    ax.plot(df['threshold'], df['precision'], label='Precision', linewidth=2)\n",
    "    ax.plot(df['threshold'], df['recall'], label='Recall', linewidth=2)\n",
    "    ax.plot(df['threshold'], df['f1'], label='F1', linewidth=2)\n",
    "    \n",
    "    # Mark optimal thresholds\n",
    "    for obj_name, obj_result in threshold_results.items():\n",
    "        if obj_name != '_threshold_curve':\n",
    "            thresh = obj_result['threshold']\n",
    "            ax.axvline(x=thresh, color='red', linestyle='--', alpha=0.5)\n",
    "            ax.text(thresh, 0.95, obj_name, rotation=90, \n",
    "                   verticalalignment='top', fontsize=8)\n",
    "    \n",
    "    ax.set_xlabel('Threshold')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Metrics vs Threshold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "print(\"✓ Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 11: Main Pipeline Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Main pipeline orchestrator defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN PIPELINE ORCHESTRATOR\n",
    "# ============================================================================\n",
    "\n",
    "def train_single_cohort_model(cohort_name: str, label: str, \n",
    "                             train_data: pd.DataFrame, oot_data: pd.DataFrame,\n",
    "                             model_names: List[str], task_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Train models for a single cohort.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - candidate_results: Results for all candidate models\n",
    "    - best_model_name: Name of best model\n",
    "    - best_model: Best model object\n",
    "    - best_metrics: Metrics for best model\n",
    "    - feature_selection_info: Feature selection metadata\n",
    "    - shap_values: SHAP values for best model (for tree-based models)\n",
    "    - coeff_table: Coefficient / odds-ratio table (for logistic models)\n",
    "    - threshold_results: Threshold optimization results\n",
    "    \"\"\"\n",
    "    if VERBOSE:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Cohort: {cohort_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "    \n",
    "    # Split features and labels\n",
    "    X_train, y_train, train_ids = split_features_labels(train_data, label)\n",
    "    X_oot, y_oot, oot_ids = split_features_labels(oot_data, label)\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(f\"\\nData sizes:\")\n",
    "        print(f\"  Train: {len(X_train):,} samples, {X_train.shape[1]} features\")\n",
    "        print(f\"  OOT: {len(X_oot):,} samples\")\n",
    "        print(f\"  Label prevalence (train): {y_train.mean():.2%}\")\n",
    "    \n",
    "    # Feature selection\n",
    "    X_train_selected, feature_selection_info = run_feature_selection(\n",
    "        X_train, y_train, FEATURE_CONFIG\n",
    "    )\n",
    "    X_oot_selected = X_oot[feature_selection_info['feature_names']]\n",
    "    \n",
    "    # Train candidate models\n",
    "    candidate_results = {}\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        if VERBOSE:\n",
    "            print(f\"\\n{'─'*60}\")\n",
    "            print(f\"Training: {model_name}\")\n",
    "            print(f\"{'─'*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Train with CV grid search\n",
    "            model, cv_results = run_cv_grid_search(\n",
    "                X_train_selected, y_train, model_name, task_type, MODEL_CONFIG\n",
    "            )\n",
    "            \n",
    "            # Calibrate if classification\n",
    "            if task_type == 'classification':\n",
    "                model = calibrate_model(\n",
    "                    model, X_train_selected, y_train, \n",
    "                    MODEL_CONFIG, model_name, task_type\n",
    "                )\n",
    "            \n",
    "            # Get predictions\n",
    "            if task_type == 'classification':\n",
    "                train_pred = model.predict_proba(X_train_selected)[:, 1]\n",
    "                oot_pred = model.predict_proba(X_oot_selected)[:, 1]\n",
    "            else:\n",
    "                train_pred = model.predict(X_train_selected)\n",
    "                oot_pred = model.predict(X_oot_selected)\n",
    "            \n",
    "            # Evaluate\n",
    "            if task_type == 'classification':\n",
    "                train_metrics = evaluate_classification(\n",
    "                    y_train.values, train_pred, EVAL_CONFIG\n",
    "                )\n",
    "                oot_metrics = evaluate_classification(\n",
    "                    y_oot.values, oot_pred, EVAL_CONFIG\n",
    "                )\n",
    "                \n",
    "                if VERBOSE:\n",
    "                    print(f\"  Train AUC: {train_metrics['auc']:.4f} | \"\n",
    "                          f\"PR-AUC: {train_metrics['pr_auc']:.4f}\")\n",
    "                    print(f\"  OOT   AUC: {oot_metrics['auc']:.4f} | \"\n",
    "                          f\"PR-AUC: {oot_metrics['pr_auc']:.4f}\")\n",
    "            else:\n",
    "                train_metrics = evaluate_regression(\n",
    "                    y_train.values, train_pred, EVAL_CONFIG\n",
    "                )\n",
    "                oot_metrics = evaluate_regression(\n",
    "                    y_oot.values, oot_pred, EVAL_CONFIG\n",
    "                )\n",
    "                \n",
    "                if VERBOSE:\n",
    "                    print(f\"  Train RMSE: {train_metrics['rmse']:.4f} | \"\n",
    "                          f\"R²: {train_metrics['r2']:.4f}\")\n",
    "                    print(f\"  OOT   RMSE: {oot_metrics['rmse']:.4f} | \"\n",
    "                          f\"R²: {oot_metrics['r2']:.4f}\")\n",
    "            \n",
    "            # Store results\n",
    "            candidate_results[model_name] = {\n",
    "                'model': model,\n",
    "                'cv_results': cv_results,\n",
    "                'metrics': {\n",
    "                    'train': train_metrics,\n",
    "                    'oot': oot_metrics\n",
    "                },\n",
    "                'predictions': {\n",
    "                    'train': pd.DataFrame({\n",
    "                        'donor_id': train_ids,\n",
    "                        'actual': y_train,\n",
    "                        'predicted': train_pred\n",
    "                    }),\n",
    "                    'oot': pd.DataFrame({\n",
    "                        'donor_id': oot_ids,\n",
    "                        'actual': y_oot,\n",
    "                        'predicted': oot_pred\n",
    "                    })\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️  {model_name} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not candidate_results:\n",
    "        raise RuntimeError(\"All models failed to train\")\n",
    "    \n",
    "    # Optional: Create ensemble\n",
    "    if ENABLE_ENSEMBLE and len(candidate_results) > 1:\n",
    "        if VERBOSE:\n",
    "            print(f\"\\n{'─'*60}\")\n",
    "            print(f\"Evaluating ensemble\")\n",
    "            print(f\"{'─'*60}\")\n",
    "        \n",
    "        # Prepare model performances for ensemble decision\n",
    "        model_performances = {}\n",
    "        for name, results in candidate_results.items():\n",
    "            if task_type == 'classification':\n",
    "                primary_metric = results['metrics']['oot']['auc']\n",
    "            else:\n",
    "                primary_metric = results['metrics']['oot']['rmse']\n",
    "            \n",
    "            model_performances[name] = {\n",
    "                'primary_metric': primary_metric\n",
    "            }\n",
    "        \n",
    "        # Create ensemble if appropriate\n",
    "        ensemble_config_full = MODEL_CONFIG.copy()\n",
    "        ensemble_config_full['ensemble'] = {'enabled': True, 'weighting_strategy': {'method': 'performance'}, 'similarity_threshold': 0.02}\n",
    "        \n",
    "        ensemble_model = create_ensemble(\n",
    "            {name: res['model'] for name, res in candidate_results.items()},\n",
    "            model_performances,\n",
    "            ensemble_config_full,\n",
    "            task_type\n",
    "        )\n",
    "        \n",
    "        if ensemble_model is not None:\n",
    "            # Evaluate ensemble\n",
    "            if task_type == 'classification':\n",
    "                train_pred = ensemble_model.predict_proba(X_train_selected)[:, 1]\n",
    "                oot_pred = ensemble_model.predict_proba(X_oot_selected)[:, 1]\n",
    "            else:\n",
    "                train_pred = ensemble_model.predict(X_train_selected)\n",
    "                oot_pred = ensemble_model.predict(X_oot_selected)\n",
    "            \n",
    "            if task_type == 'classification':\n",
    "                train_metrics = evaluate_classification(\n",
    "                    y_train.values, train_pred, EVAL_CONFIG\n",
    "                )\n",
    "                oot_metrics = evaluate_classification(\n",
    "                    y_oot.values, oot_pred, EVAL_CONFIG\n",
    "                )\n",
    "                \n",
    "                if VERBOSE:\n",
    "                    print(f\"  Ensemble Train AUC: {train_metrics['auc']:.4f}\")\n",
    "                    print(f\"  Ensemble OOT   AUC: {oot_metrics['auc']:.4f}\")\n",
    "            else:\n",
    "                train_metrics = evaluate_regression(\n",
    "                    y_train.values, train_pred, EVAL_CONFIG\n",
    "                )\n",
    "                oot_metrics = evaluate_regression(\n",
    "                    y_oot.values, oot_pred, EVAL_CONFIG\n",
    "                )\n",
    "                \n",
    "                if VERBOSE:\n",
    "                    print(f\"  Ensemble Train RMSE: {train_metrics['rmse']:.4f}\")\n",
    "                    print(f\"  Ensemble OOT   RMSE: {oot_metrics['rmse']:.4f}\")\n",
    "            \n",
    "            # Add ensemble to candidates\n",
    "            candidate_results['ensemble'] = {\n",
    "                'model': ensemble_model,\n",
    "                'cv_results': {'cv_enabled': False},\n",
    "                'metrics': {\n",
    "                    'train': train_metrics,\n",
    "                    'oot': oot_metrics\n",
    "                },\n",
    "                'predictions': {\n",
    "                    'train': pd.DataFrame({\n",
    "                        'donor_id': train_ids,\n",
    "                        'actual': y_train,\n",
    "                        'predicted': train_pred\n",
    "                    }),\n",
    "                    'oot': pd.DataFrame({\n",
    "                        'donor_id': oot_ids,\n",
    "                        'actual': y_oot,\n",
    "                        'predicted': oot_pred\n",
    "                    })\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    # Select best model\n",
    "    if VERBOSE:\n",
    "        print(f\"\\n{'─'*60}\")\n",
    "        print(f\"Selecting best model\")\n",
    "        print(f\"{'─'*60}\")\n",
    "    \n",
    "    best_model_name = select_best_model(candidate_results, task_type, MODEL_CONFIG)\n",
    "    best_results = candidate_results[best_model_name]\n",
    "    \n",
    "    # Interpretability: SHAP for trees, odds table for logistic\n",
    "    shap_values = None\n",
    "    coeff_table = None\n",
    "    \n",
    "    if best_model_name != 'ensemble':\n",
    "        # Start from the stored model (may be a CalibratedClassifierCV or a Pipeline)\n",
    "        model_obj = best_results['model']\n",
    "        \n",
    "        # If calibrated, unwrap to the base_estimator (usually a Pipeline)\n",
    "        if isinstance(model_obj, CalibratedClassifierCV):\n",
    "            base_estimator = model_obj.estimator\n",
    "        else:\n",
    "            base_estimator = model_obj\n",
    "        \n",
    "        # If it's a Pipeline, pull out the final estimator\n",
    "        if hasattr(base_estimator, \"named_steps\") and \"model\" in base_estimator.named_steps:\n",
    "            actual_model = base_estimator.named_steps[\"model\"]\n",
    "        else:\n",
    "            actual_model = base_estimator\n",
    "        \n",
    "        # LogisticRegression: use coefficients / odds ratios instead of SHAP\n",
    "        if isinstance(actual_model, LogisticRegression) and task_type == 'classification':\n",
    "            if VERBOSE:\n",
    "                print(\"Using logistic regression coefficients / odds ratios instead of SHAP.\")\n",
    "            coeff_table = build_logistic_coeff_table(\n",
    "                base_estimator,   # pipeline with preprocessor + model\n",
    "                top_n=30,\n",
    "                round_digits=3,\n",
    "            )\n",
    "        else:\n",
    "            # Tree or other non-linear model: compute SHAP on base_estimator\n",
    "            shap_values = generate_shap_values(\n",
    "                base_estimator, X_train_selected, EVAL_CONFIG\n",
    "            )\n",
    "    \n",
    "    # Optimize thresholds (classification only)\n",
    "    threshold_results = {}\n",
    "    if task_type == 'classification':\n",
    "        threshold_results = optimize_thresholds(\n",
    "            y_oot.values,\n",
    "            best_results['predictions']['oot']['predicted'].values,\n",
    "            EVAL_CONFIG\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        'candidate_results': candidate_results,\n",
    "        'best_model_name': best_model_name,\n",
    "        'best_model': best_results['model'],\n",
    "        'best_metrics': best_results['metrics'],\n",
    "        'best_predictions': best_results['predictions'],\n",
    "        'feature_selection_info': feature_selection_info,\n",
    "        'shap_values': shap_values,\n",
    "        'coeff_table': coeff_table,\n",
    "        'threshold_results': threshold_results,\n",
    "        'X_train': X_train_selected,\n",
    "        'X_oot': X_oot_selected,\n",
    "        'y_train': y_train,\n",
    "        'y_oot': y_oot\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ Main pipeline orchestrator defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 12: Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DONOR MODELING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "Loading data...\n",
      "✓ Data loaded\n",
      "  Train: (662474, 219)\n",
      "  OOT: (1037432, 230)\n",
      "\n",
      "Label: label_will_give_during_giving_tuesday\n",
      "Task type: classification\n",
      "\n",
      "Cohorts to run: all_donors\n",
      "Models to run: logreg, lgbm\n",
      "  Cohort 'all_donors': 658,595 donors (99.4%)\n",
      "  Cohort 'all_donors': 1,036,822 donors (99.9%)\n",
      "\n",
      "================================================================================\n",
      "Cohort: all_donors\n",
      "================================================================================\n",
      "  Features: 154 columns\n",
      "  Labels: 658595 samples\n",
      "  Excluded: 65 columns (53 label columns)\n",
      "  Features: 165 columns\n",
      "  Labels: 1036822 samples\n",
      "  Excluded: 65 columns (53 label columns)\n",
      "\n",
      "Data sizes:\n",
      "  Train: 658,595 samples, 154 features\n",
      "  OOT: 1,036,822 samples\n",
      "  Label prevalence (train): 4.03%\n",
      "\n",
      "Feature Selection Pipeline:\n",
      "  Starting features: 154\n",
      "  Removed 16 correlated features (>0.95)\n",
      "  Selected 58 features via tree importance\n",
      "  Top 5 features: email_open_rate_3m, days_since_last_gift, teacher_still_available_during_range, email_open_rate_velocity_3m_vs_12m, pct_amount_green_lifetime\n",
      "  Removed 2 low-variance features (<0.01)\n",
      "  Final features: 56\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Training: logreg\n",
      "────────────────────────────────────────────────────────────\n",
      "    Running 5-fold CV over 8 parameter combinations...\n",
      "    Best CV score: 0.8636\n",
      "    Best params: {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "    Applying isotonic calibration...\n",
      "  Train AUC: 0.8643 | PR-AUC: 0.2598\n",
      "  OOT   AUC: 0.8584 | PR-AUC: 0.2155\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Training: lgbm\n",
      "────────────────────────────────────────────────────────────\n",
      "    Using 'sparse_label' grid (prevalence=4.03%)\n",
      "    Running 5-fold CV over 10 random combinations (from 32 possible)...\n",
      "    Best CV score: 0.9310\n",
      "    Best params: {'subsample': 0.8, 'reg_lambda': 2.0, 'reg_alpha': 1.0, 'num_leaves': 15, 'n_estimators': 800, 'min_child_samples': 300, 'max_depth': 4, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "    Applying isotonic calibration...\n",
      "  Train AUC: 0.9405 | PR-AUC: 0.5164\n",
      "  OOT   AUC: 0.7723 | PR-AUC: 0.1951\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Selecting best model\n",
      "────────────────────────────────────────────────────────────\n",
      "  Best model: logreg (oot_auc=0.8584)\n",
      "Using logistic regression coefficients / odds ratios instead of SHAP.\n",
      "\n",
      "  Threshold Optimization:\n",
      "    f1_optimal: threshold=0.198\n",
      "    precision_target: threshold=0.673\n",
      "    recall_target: threshold=0.069\n",
      "  ✓ Saved logistic regression coefficients table\n",
      "\n",
      "✓ Results saved to: /Users/matt.fritz/Desktop/modeling_results/label_will_give_during_giving_tuesday/all_donors/best_model\n",
      "\n",
      "================================================================================\n",
      "PIPELINE COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Results saved to: /Users/matt.fritz/Desktop/modeling_results/label_will_give_during_giving_tuesday/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXECUTE PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "# Load data\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DONOR MODELING PIPELINE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nLoading data...\")\n",
    "\n",
    "train_features, oot_features = load_data(TRAIN_FEATURES_PATH, OOT_FEATURES_PATH)\n",
    "\n",
    "print(f\"✓ Data loaded\")\n",
    "print(f\"  Train: {train_features.shape}\")\n",
    "print(f\"  OOT: {oot_features.shape}\")\n",
    "\n",
    "# Validate label\n",
    "if LABEL not in train_features.columns:\n",
    "    available_labels = sorted([c for c in train_features.columns if c.startswith('label_')])\n",
    "    print(f\"\\n⚠️  Label '{LABEL}' not found!\")\n",
    "    print(f\"\\nAvailable labels:\")\n",
    "    for lbl in available_labels[:10]:\n",
    "        print(f\"  - {lbl}\")\n",
    "    if len(available_labels) > 10:\n",
    "        print(f\"  ... and {len(available_labels) - 10} more\")\n",
    "    raise ValueError(f\"Label '{LABEL}' not found\")\n",
    "\n",
    "# Detect task type\n",
    "task_type = detect_task_type(train_features[LABEL])\n",
    "print(f\"\\nLabel: {LABEL}\")\n",
    "print(f\"Task type: {task_type}\")\n",
    "\n",
    "# Determine which cohorts to run\n",
    "if COHORTS_TO_RUN is None:\n",
    "    cohorts_to_run = list(COHORT_CONFIG['cohorts'].keys())\n",
    "else:\n",
    "    cohorts_to_run = COHORTS_TO_RUN\n",
    "\n",
    "# Validate cohorts\n",
    "for cohort in cohorts_to_run:\n",
    "    if cohort not in COHORT_CONFIG['cohorts']:\n",
    "        raise ValueError(f\"Cohort '{cohort}' not found in config\")\n",
    "\n",
    "print(f\"\\nCohorts to run: {', '.join(cohorts_to_run)}\")\n",
    "\n",
    "# Determine which models to run\n",
    "if MODELS_TO_RUN is None:\n",
    "    if task_type == 'classification':\n",
    "        models_to_run = [name for name, config in MODEL_CONFIG.get('classification', {}).items()\n",
    "                        if config.get('enabled', False)]\n",
    "    else:\n",
    "        models_to_run = [name for name, config in MODEL_CONFIG.get('regression', {}).items()\n",
    "                        if config.get('enabled', False)]\n",
    "else:\n",
    "    models_to_run = MODELS_TO_RUN\n",
    "\n",
    "print(f\"Models to run: {', '.join(models_to_run)}\")\n",
    "\n",
    "# Run pipeline for each cohort\n",
    "all_cohort_results = {}\n",
    "\n",
    "for cohort_name in cohorts_to_run:\n",
    "    # Filter to cohort\n",
    "    train_cohort = apply_cohort_filter(train_features, cohort_name, COHORT_CONFIG)\n",
    "    oot_cohort = apply_cohort_filter(oot_features, cohort_name, COHORT_CONFIG)\n",
    "    \n",
    "    # Train models\n",
    "    cohort_results = train_single_cohort_model(\n",
    "        cohort_name, LABEL, train_cohort, oot_cohort,\n",
    "        models_to_run, task_type\n",
    "    )\n",
    "    \n",
    "    all_cohort_results[cohort_name] = cohort_results\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = Path(OUTPUT_BASE_DIR) / LABEL / cohort_name / 'best_model'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    plots_dir = output_dir / 'plots'\n",
    "    plots_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    with open(output_dir / 'model.pkl', 'wb') as f:\n",
    "        pickle.dump(cohort_results['best_model'], f)\n",
    "    \n",
    "    # Save config\n",
    "    config_info = {\n",
    "        'label': LABEL,\n",
    "        'cohort': cohort_name,\n",
    "        'task_type': task_type,\n",
    "        'best_model_name': cohort_results['best_model_name'],\n",
    "        'models_evaluated': list(cohort_results['candidate_results'].keys()),\n",
    "        'n_features': len(cohort_results['feature_selection_info']['feature_names']),\n",
    "        'feature_names': cohort_results['feature_selection_info']['feature_names'],\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    with open(output_dir / 'config.json', 'w') as f:\n",
    "        json.dump(config_info, f, indent=2)\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_flat = {}\n",
    "    for split in ['train', 'oot']:\n",
    "        for metric, value in cohort_results['best_metrics'][split].items():\n",
    "            if metric != 'lift_table':\n",
    "                metrics_flat[f'{split}_{metric}'] = value\n",
    "    \n",
    "    with open(output_dir / 'metrics.json', 'w') as f:\n",
    "        json.dump(metrics_flat, f, indent=2)\n",
    "    \n",
    "    # Save predictions\n",
    "    cohort_results['best_predictions']['train'].to_csv(\n",
    "        output_dir / 'predictions_train.csv', index=False\n",
    "    )\n",
    "    cohort_results['best_predictions']['oot'].to_csv(\n",
    "        output_dir / 'predictions_oot.csv', index=False\n",
    "    )\n",
    "    \n",
    "    # Save SHAP values\n",
    "    if cohort_results['shap_values'] is not None:\n",
    "        shap_df = pd.DataFrame(\n",
    "            cohort_results['shap_values']['values'],\n",
    "            columns=cohort_results['shap_values']['feature_names']\n",
    "        )\n",
    "        shap_df.to_csv(output_dir / 'shap_values.csv', index=False)\n",
    "        \n",
    "        # SHAP direction summary\n",
    "        shap_direction = summarize_shap_direction(cohort_results['shap_values'])\n",
    "        if shap_direction is not None:\n",
    "            shap_direction.to_csv(output_dir / 'shap_direction_summary.csv', index=False)\n",
    "\n",
    "    # Save coefficient table (for logistic regression)\n",
    "    if cohort_results['coeff_table'] is not None:\n",
    "        cohort_results['coeff_table'].to_csv(\n",
    "            output_dir / 'logistic_coefficients.csv', \n",
    "            index=False\n",
    "        )\n",
    "        print(f\"  ✓ Saved logistic regression coefficients table\")\n",
    "    \n",
    "    # Save threshold results\n",
    "    if cohort_results['threshold_results']:\n",
    "        threshold_save = {k: v for k, v in cohort_results['threshold_results'].items()\n",
    "                         if k != '_threshold_curve'}\n",
    "        with open(output_dir / 'threshold_analysis.json', 'w') as f:\n",
    "            json.dump(threshold_save, f, indent=2)\n",
    "    \n",
    "    # Generate plots\n",
    "    if task_type == 'classification':\n",
    "        # Lift curve\n",
    "        lift_table = cohort_results['best_metrics']['oot']['lift_table']\n",
    "        plot_lift_curve(lift_table, plots_dir / 'lift_curve.png')\n",
    "        \n",
    "        # Calibration\n",
    "        plot_calibration_curve(\n",
    "            cohort_results['y_oot'].values,\n",
    "            cohort_results['best_predictions']['oot']['predicted'].values,\n",
    "            plots_dir / 'calibration.png'\n",
    "        )\n",
    "        \n",
    "        # Threshold analysis\n",
    "        if cohort_results['threshold_results']:\n",
    "            plot_threshold_analysis(\n",
    "                cohort_results['threshold_results'],\n",
    "                plots_dir / 'threshold_analysis.png'\n",
    "            )\n",
    "    \n",
    "    # SHAP plot\n",
    "    if cohort_results['shap_values'] is not None:\n",
    "        plot_shap_summary(\n",
    "            cohort_results['shap_values'],\n",
    "            plots_dir / 'shap_summary.png',\n",
    "            top_n=EVAL_CONFIG.get('shap', {}).get('top_features', 20)\n",
    "        )\n",
    "    \n",
    "    # Save candidate models if requested\n",
    "    if SAVE_CANDIDATE_MODELS:\n",
    "        candidates_dir = Path(OUTPUT_BASE_DIR) / LABEL / cohort_name / 'candidates'\n",
    "        for model_name, results in cohort_results['candidate_results'].items():\n",
    "            model_dir = candidates_dir / model_name\n",
    "            model_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            with open(model_dir / 'model.pkl', 'wb') as f:\n",
    "                pickle.dump(results['model'], f)\n",
    "    \n",
    "    print(f\"\\n✓ Results saved to: {output_dir}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PIPELINE COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nResults saved to: {OUTPUT_BASE_DIR}/{LABEL}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 13: Results Summary & Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Label: label_will_give_during_giving_tuesday\n",
      "Task: classification\n",
      "\n",
      "    cohort best_model  n_features  train_auc  oot_auc  train_pr_auc  oot_pr_auc  oot_brier  lift_at_1pct  lift_at_5pct  lift_at_10pct\n",
      "all_donors     logreg          56   0.864339 0.858392      0.259752    0.215496   0.021928     14.886795      8.671313       5.970988\n",
      "\n",
      "================================================================================\n",
      "TOP FEATURES BY COHORT\n",
      "================================================================================\n",
      "\n",
      "all_donors:\n",
      "                             feature  importance\n",
      "                  email_open_rate_3m          94\n",
      "                days_since_last_gift          83\n",
      "teacher_still_available_during_range          42\n",
      "  email_open_rate_velocity_3m_vs_12m          42\n",
      "           pct_amount_green_lifetime          41\n",
      "                          is_teacher          19\n",
      "     latest_project_got_fully_funded          17\n",
      "         latest_gift_amount_nongreen          17\n",
      "            pct_amount_big_event_12m          15\n",
      "             pct_gifts_gift_card_12m          14\n",
      "\n",
      "================================================================================\n",
      "THRESHOLD RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "all_donors:\n",
      "  f1_optimal: 0.198\n",
      "  precision_target: 0.673\n",
      "  recall_target: 0.069\n",
      "\n",
      "================================================================================\n",
      "Pipeline execution complete!\n",
      "Results directory: /Users/matt.fritz/Desktop/modeling_results/label_will_give_during_giving_tuesday/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "DETAILED RESULTS EXAMINATION\n",
      "================================================================================\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "COHORT: all_donors\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Models evaluated: ['logreg', 'lgbm']\n",
      "Best model: logreg\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "MODEL: logreg\n",
      "⭐ SELECTED AS BEST MODEL\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Train Metrics:\n",
      "  auc: 0.8643\n",
      "  pr_auc: 0.2598\n",
      "  brier_score: 0.0334\n",
      "  log_loss: 0.1280\n",
      "  baseline_rate: 0.0403\n",
      "  lift_at_0.01: 11.0788\n",
      "  lift_at_0.05: 7.2635\n",
      "  lift_at_0.1: 5.4440\n",
      "  lift_at_0.25: 3.2033\n",
      "\n",
      "OOT Metrics:\n",
      "  auc: 0.8584\n",
      "  pr_auc: 0.2155\n",
      "  brier_score: 0.0219\n",
      "  log_loss: 0.0930\n",
      "  baseline_rate: 0.0247\n",
      "  lift_at_0.01: 14.8868\n",
      "  lift_at_0.05: 8.6713\n",
      "  lift_at_0.1: 5.9710\n",
      "  lift_at_0.25: 3.1772\n",
      "\n",
      "CV Results:\n",
      "  Best CV score: 0.8636\n",
      "  Best params: {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "MODEL: lgbm\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Train Metrics:\n",
      "  auc: 0.9405\n",
      "  pr_auc: 0.5164\n",
      "  brier_score: 0.0259\n",
      "  log_loss: 0.0945\n",
      "  baseline_rate: 0.0403\n",
      "  lift_at_0.01: 19.6227\n",
      "  lift_at_0.05: 11.4229\n",
      "  lift_at_0.1: 7.5685\n",
      "  lift_at_0.25: 3.7289\n",
      "\n",
      "OOT Metrics:\n",
      "  auc: 0.7723\n",
      "  pr_auc: 0.1951\n",
      "  brier_score: 0.0235\n",
      "  log_loss: 0.1134\n",
      "  baseline_rate: 0.0247\n",
      "  lift_at_0.01: 16.0962\n",
      "  lift_at_0.05: 8.1478\n",
      "  lift_at_0.1: 5.1779\n",
      "  lift_at_0.25: 2.6530\n",
      "\n",
      "CV Results:\n",
      "  Best CV score: 0.9310\n",
      "  Best params: {'subsample': 0.8, 'reg_lambda': 2.0, 'reg_alpha': 1.0, 'num_leaves': 15, 'n_estimators': 800, 'min_child_samples': 300, 'max_depth': 4, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "\n",
      "================================================================================\n",
      "LIFT TABLE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Model: logreg\n",
      "Cohort: all_donors\n",
      "\n",
      " quantile     n  n_events  avg_score  event_rate     lift   cum_n  cum_n_events  cum_event_rate  cum_lift  cum_pct_captured\n",
      "        1 51842   11114.0   0.280566    0.214382 8.671145   51842       11114.0        0.214382  8.671145          0.433565\n",
      "        2 51841    4192.0   0.134180    0.080863 3.270662  103683       15306.0        0.147623  5.970930          0.597098\n",
      "        3 51841    2426.0   0.086135    0.046797 1.892802  155524       17732.0        0.114015  4.611563          0.691738\n",
      "        4 51841    1491.0   0.064346    0.028761 1.163301  207365       19223.0        0.092701  3.749502          0.749902\n",
      "        5 51841    1138.0   0.047290    0.021952 0.887885  259206       20361.0        0.078551  3.177180          0.794297\n",
      "        6 51841     962.0   0.036760    0.018557 0.750567  311047       21323.0        0.068552  2.772746          0.831825\n",
      "        7 51841     838.0   0.027842    0.016165 0.653820  362888       22161.0        0.061068  2.470043          0.864516\n",
      "        8 51841     683.0   0.022762    0.013175 0.532887  414729       22844.0        0.055082  2.227899          0.891160\n",
      "        9 51841     516.0   0.017090    0.009954 0.402591  466570       23360.0        0.050068  2.025088          0.911290\n",
      "       10 51841     388.0   0.013134    0.007484 0.302724  518411       23748.0        0.045809  1.852852          0.926426\n",
      "       11 51841     303.0   0.010733    0.005845 0.236405  570252       24051.0        0.042176  1.705902          0.938246\n",
      "       12 51841     272.0   0.008904    0.005247 0.212219  622093       24323.0        0.039099  1.581429          0.948857\n",
      "       13 51841     242.0   0.006772    0.004668 0.188812  673934       24565.0        0.036450  1.474305          0.958298\n",
      "       14 51841     207.0   0.006118    0.003993 0.161505  725775       24772.0        0.034132  1.380533          0.966373\n",
      "       15 51841     165.0   0.005264    0.003183 0.128736  777616       24937.0        0.032069  1.297080          0.972810\n",
      "       16 51841     151.0   0.003905    0.002913 0.117813  829457       25088.0        0.030246  1.223376          0.978700\n",
      "       17 51841     161.0   0.003616    0.003106 0.125615  881298       25249.0        0.028650  1.158802          0.984981\n",
      "       18 51841     130.0   0.003157    0.002508 0.101428  933139       25379.0        0.027197  1.100059          0.990052\n",
      "       19 51841     139.0   0.002469    0.002681 0.108450  984980       25518.0        0.025907  1.047869          0.995475\n",
      "       20 51842     116.0   0.001948    0.002238 0.090503 1036822       25634.0        0.024724  1.000000          1.000000\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Key Insights:\n",
      "────────────────────────────────────────────────────────────\n",
      "Top 5% lift: 8.67x\n",
      "Top 5% captures: 43.4% of all events\n",
      "Lift drops below 2x at: top 50%\n",
      "\n",
      "================================================================================\n",
      "SHAP FEATURE IMPORTANCE\n",
      "================================================================================\n",
      "⚠️  SHAP values not available (may be disabled or ensemble model)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "LOGISTIC REGRESSION COEFFICIENTS\n",
      "================================================================================\n",
      "\n",
      "Model: logreg\n",
      "Cohort: all_donors\n",
      "\n",
      "Top Features by Absolute Coefficient:\n",
      "                                    feature  coefficient  odds_ratio                                                                                                                                                                  interpretation\n",
      "  num__teacher_still_available_during_range        0.659       1.934   A 1-unit increase in 'num__teacher_still_available_during_range' is associated with approximately 93.4% higher odds of the positive outcome, holding other features constant.\n",
      "                         num__gift_count_3m        0.534       1.706                          A 1-unit increase in 'num__gift_count_3m' is associated with approximately 70.6% higher odds of the positive outcome, holding other features constant.\n",
      "             num__pct_amount_green_lifetime       -0.480       0.619               A 1-unit increase in 'num__pct_amount_green_lifetime' is associated with approximately 38.1% lower odds of the positive outcome, holding other features constant.\n",
      "                  num__gift_count_green_12m        0.420       1.522                   A 1-unit increase in 'num__gift_count_green_12m' is associated with approximately 52.2% higher odds of the positive outcome, holding other features constant.\n",
      "               num__pct_gifts_gift_card_12m       -0.378       0.685                 A 1-unit increase in 'num__pct_gifts_gift_card_12m' is associated with approximately 31.5% lower odds of the positive outcome, holding other features constant.\n",
      "                       num__entropy_teacher        0.358       1.431                        A 1-unit increase in 'num__entropy_teacher' is associated with approximately 43.1% higher odds of the positive outcome, holding other features constant.\n",
      "       num__latest_project_got_fully_funded       -0.358       0.699         A 1-unit increase in 'num__latest_project_got_fully_funded' is associated with approximately 30.1% lower odds of the positive outcome, holding other features constant.\n",
      "           num__latest_is_giftcard_purchase       -0.357       0.700             A 1-unit increase in 'num__latest_is_giftcard_purchase' is associated with approximately 30.0% lower odds of the positive outcome, holding other features constant.\n",
      "               num__pct_amount_in_top_month       -0.325       0.723                 A 1-unit increase in 'num__pct_amount_in_top_month' is associated with approximately 27.7% lower odds of the positive outcome, holding other features constant.\n",
      "      cat__latest_project_grade_Grades 9-12       -0.320       0.726        A 1-unit increase in 'cat__latest_project_grade_Grades 9-12' is associated with approximately 27.4% lower odds of the positive outcome, holding other features constant.\n",
      "       cat__latest_project_grade_Grades 6-8       -0.320       0.726         A 1-unit increase in 'cat__latest_project_grade_Grades 6-8' is associated with approximately 27.4% lower odds of the positive outcome, holding other features constant.\n",
      "                            num__is_teacher       -0.297       0.743                              A 1-unit increase in 'num__is_teacher' is associated with approximately 25.7% lower odds of the positive outcome, holding other features constant.\n",
      "num__has_given_during_giving_tuesday_before        0.296       1.344 A 1-unit increase in 'num__has_given_during_giving_tuesday_before' is associated with approximately 34.4% higher odds of the positive outcome, holding other features constant.\n",
      "       cat__latest_project_grade_Grades 3-5       -0.242       0.785         A 1-unit increase in 'cat__latest_project_grade_Grades 3-5' is associated with approximately 21.5% lower odds of the positive outcome, holding other features constant.\n",
      "                   num__num_unique_teachers       -0.218       0.804                     A 1-unit increase in 'num__num_unique_teachers' is associated with approximately 19.6% lower odds of the positive outcome, holding other features constant.\n",
      "                  num__days_since_last_gift        0.180       1.197                   A 1-unit increase in 'num__days_since_last_gift' is associated with approximately 19.7% higher odds of the positive outcome, holding other features constant.\n",
      "          num__pct_gifts_gift_card_lifetime        0.176       1.193           A 1-unit increase in 'num__pct_gifts_gift_card_lifetime' is associated with approximately 19.3% higher odds of the positive outcome, holding other features constant.\n",
      "               num__share_active_months_12m        0.171       1.186                A 1-unit increase in 'num__share_active_months_12m' is associated with approximately 18.6% higher odds of the positive outcome, holding other features constant.\n",
      "             num__pct_projects_fully_funded        0.154       1.166              A 1-unit increase in 'num__pct_projects_fully_funded' is associated with approximately 16.6% higher odds of the positive outcome, holding other features constant.\n",
      "                    num__email_open_rate_3m        0.151       1.163                     A 1-unit increase in 'num__email_open_rate_3m' is associated with approximately 16.3% higher odds of the positive outcome, holding other features constant.\n",
      "                   num__share_gap_mean_days        0.151       1.163                    A 1-unit increase in 'num__share_gap_mean_days' is associated with approximately 16.3% higher odds of the positive outcome, holding other features constant.\n",
      "                 num__zip_median_home_value        0.144       1.155                  A 1-unit increase in 'num__zip_median_home_value' is associated with approximately 15.5% higher odds of the positive outcome, holding other features constant.\n",
      "          num__max_donation_sequence_number        0.105       1.111           A 1-unit increase in 'num__max_donation_sequence_number' is associated with approximately 11.1% higher odds of the positive outcome, holding other features constant.\n",
      "                num__pct_amount_on_weekends       -0.101       0.904                   A 1-unit increase in 'num__pct_amount_on_weekends' is associated with approximately 9.6% lower odds of the positive outcome, holding other features constant.\n",
      "          cat__latest_project_grade_missing        0.098       1.103           A 1-unit increase in 'cat__latest_project_grade_missing' is associated with approximately 10.3% higher odds of the positive outcome, holding other features constant.\n",
      "              num__pct_amount_big_event_12m        0.090       1.094                A 1-unit increase in 'num__pct_amount_big_event_12m' is associated with approximately 9.4% higher odds of the positive outcome, holding other features constant.\n",
      "                        num__gift_count_12m       -0.089       0.915                           A 1-unit increase in 'num__gift_count_12m' is associated with approximately 8.5% lower odds of the positive outcome, holding other features constant.\n",
      "                  num__first_donation_month        0.083       1.087                    A 1-unit increase in 'num__first_donation_month' is associated with approximately 8.7% higher odds of the positive outcome, holding other features constant.\n",
      "               num__is_marketing_subscribed        0.082       1.086                 A 1-unit increase in 'num__is_marketing_subscribed' is associated with approximately 8.6% higher odds of the positive outcome, holding other features constant.\n",
      "            num__share_month_coverage_ratio        0.078       1.082              A 1-unit increase in 'num__share_month_coverage_ratio' is associated with approximately 8.2% higher odds of the positive outcome, holding other features constant.\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Summary:\n",
      "────────────────────────────────────────────────────────────\n",
      "Total features: 30\n",
      "Positive coefficients (increase odds): 18\n",
      "Negative coefficients (decrease odds): 12\n",
      "\n",
      "Strongest positive effect: num__teacher_still_available_during_range\n",
      "  Odds ratio: 1.934\n",
      "\n",
      "Strongest negative effect: num__pct_amount_green_lifetime\n",
      "  Odds ratio: 0.619\n",
      "THRESHOLD OPTIMIZATION\n",
      "================================================================================\n",
      "\n",
      "Model: logreg\n",
      "Cohort: all_donors\n",
      "\n",
      "Optimal Thresholds:\n",
      "\n",
      "f1_optimal:\n",
      "  Threshold: 0.198\n",
      "  f1: 0.2893\n",
      "\n",
      "precision_target:\n",
      "  Threshold: 0.673\n",
      "  precision: 0.7844\n",
      "  target: 0.8000\n",
      "\n",
      "recall_target:\n",
      "  Threshold: 0.069\n",
      "  recall: 0.7048\n",
      "  target: 0.7000\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Threshold Sensitivity:\n",
      "────────────────────────────────────────────────────────────\n",
      "Threshold    Precision    Recall       F1          \n",
      "──────────────────────────────────────────────────\n",
      "0.10         0.147        0.600        0.236       \n",
      "0.30         0.313        0.220        0.258       \n",
      "0.50         0.612        0.041        0.077       \n",
      "0.70         0.784        0.011        0.022       \n",
      "0.90         0.971        0.001        0.003       \n",
      "\n",
      "================================================================================\n",
      "PREDICTION DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Model: logreg\n",
      "Cohort: all_donors\n",
      "\n",
      "Predicted Probability Distribution:\n",
      "  Min:    0.0000\n",
      "  25th %: 0.0042\n",
      "  Median: 0.0120\n",
      "  75th %: 0.0435\n",
      "  Max:    1.0000\n",
      "\n",
      "Actual Event Rate by Score Bucket:\n",
      "(Created 10 buckets)\n",
      "              actual         predicted\n",
      "               count    mean      mean\n",
      "score_bucket                          \n",
      "D1            122893  0.0024    0.0023\n",
      "D10            99896  0.1510    0.2114\n",
      "D2            111944  0.0029    0.0036\n",
      "D3            131765  0.0035    0.0054\n",
      "D4             59759  0.0051    0.0070\n",
      "D5            116338  0.0058    0.0105\n",
      "D6             80133  0.0092    0.0161\n",
      "D7            110248  0.0148    0.0257\n",
      "D8             98135  0.0206    0.0431\n",
      "D9            105711  0.0387    0.0765\n",
      "\n",
      "================================================================================\n",
      "END OF DETAILED EXAMINATION\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RESULTS SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "# Create summary table\n",
    "summary_rows = []\n",
    "\n",
    "for cohort_name, results in all_cohort_results.items():\n",
    "    row = {\n",
    "        'cohort': cohort_name,\n",
    "        'best_model': results['best_model_name'],\n",
    "        'n_features': len(results['feature_selection_info']['feature_names'])\n",
    "    }\n",
    "    \n",
    "    # Add metrics\n",
    "    if task_type == 'classification':\n",
    "        row['train_auc'] = results['best_metrics']['train']['auc']\n",
    "        row['oot_auc'] = results['best_metrics']['oot']['auc']\n",
    "        row['train_pr_auc'] = results['best_metrics']['train']['pr_auc']\n",
    "        row['oot_pr_auc'] = results['best_metrics']['oot']['pr_auc']\n",
    "        row['oot_brier'] = results['best_metrics']['oot']['brier_score']\n",
    "        \n",
    "        # Add lift@K\n",
    "        for k in [0.01, 0.05, 0.10]:\n",
    "            key = f'lift_at_{k}'\n",
    "            if key in results['best_metrics']['oot']:\n",
    "                row[f'lift_at_{int(k*100)}pct'] = results['best_metrics']['oot'][key]\n",
    "    else:\n",
    "        row['train_rmse'] = results['best_metrics']['train']['rmse']\n",
    "        row['oot_rmse'] = results['best_metrics']['oot']['rmse']\n",
    "        row['train_r2'] = results['best_metrics']['train']['r2']\n",
    "        row['oot_r2'] = results['best_metrics']['oot']['r2']\n",
    "        row['oot_mae'] = results['best_metrics']['oot']['mae']\n",
    "    \n",
    "    summary_rows.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nLabel: {LABEL}\")\n",
    "print(f\"Task: {task_type}\")\n",
    "print(f\"\\n{summary_df.to_string(index=False)}\")\n",
    "\n",
    "# Show top features for each cohort\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TOP FEATURES BY COHORT\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for cohort_name, results in all_cohort_results.items():\n",
    "    print(f\"\\n{cohort_name}:\")\n",
    "    \n",
    "    if results['shap_values'] is not None:\n",
    "        shap_direction = summarize_shap_direction(results['shap_values'], top_n=10)\n",
    "        if shap_direction is not None:\n",
    "            print(shap_direction[['feature', 'mean_abs_shap', 'direction']].to_string(index=False))\n",
    "    else:\n",
    "        importance_df = results['feature_selection_info']['importance_scores']\n",
    "        if importance_df is not None:\n",
    "            print(importance_df.head(10)[['feature', 'importance']].to_string(index=False))\n",
    "\n",
    "# Show threshold recommendations (classification only)\n",
    "if task_type == 'classification':\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"THRESHOLD RECOMMENDATIONS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for cohort_name, results in all_cohort_results.items():\n",
    "        if results['threshold_results']:\n",
    "            print(f\"\\n{cohort_name}:\")\n",
    "            for obj_name, obj_result in results['threshold_results'].items():\n",
    "                if obj_name != '_threshold_curve':\n",
    "                    print(f\"  {obj_name}: {obj_result['threshold']:.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Pipeline execution complete!\")\n",
    "print(f\"Results directory: {OUTPUT_BASE_DIR}/{LABEL}/\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DETAILED RESULTS EXAMINATION\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DETAILED RESULTS EXAMINATION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Select cohort to examine\n",
    "COHORT_TO_EXAMINE = 'all_donors'  # Change this to examine different cohorts\n",
    "\n",
    "if COHORT_TO_EXAMINE not in all_cohort_results:\n",
    "    print(f\"⚠️  Cohort '{COHORT_TO_EXAMINE}' not found\")\n",
    "    print(f\"Available cohorts: {list(all_cohort_results.keys())}\")\n",
    "else:\n",
    "    cohort_results = all_cohort_results[COHORT_TO_EXAMINE]\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\"COHORT: {COHORT_TO_EXAMINE}\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    # Show all candidate models and their metrics\n",
    "    print(f\"\\nModels evaluated: {list(cohort_results['candidate_results'].keys())}\")\n",
    "    print(f\"Best model: {cohort_results['best_model_name']}\")\n",
    "    \n",
    "    for model_name, model_results in cohort_results['candidate_results'].items():\n",
    "        print(f\"\\n{'─'*60}\")\n",
    "        print(f\"MODEL: {model_name}\")\n",
    "        if model_name == cohort_results['best_model_name']:\n",
    "            print(\"⭐ SELECTED AS BEST MODEL\")\n",
    "        print(f\"{'─'*60}\")\n",
    "        \n",
    "        metrics = model_results['metrics']\n",
    "        \n",
    "        # Show train metrics\n",
    "        print(\"\\nTrain Metrics:\")\n",
    "        for k, v in metrics['train'].items():\n",
    "            if k != 'lift_table':\n",
    "                if isinstance(v, float):\n",
    "                    print(f\"  {k}: {v:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  {k}: {v}\")\n",
    "        \n",
    "        # Show OOT metrics\n",
    "        print(\"\\nOOT Metrics:\")\n",
    "        for k, v in metrics['oot'].items():\n",
    "            if k != 'lift_table':\n",
    "                if isinstance(v, float):\n",
    "                    print(f\"  {k}: {v:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  {k}: {v}\")\n",
    "        \n",
    "        # Show CV info if available\n",
    "        if model_results['cv_results']['cv_enabled']:\n",
    "            print(\"\\nCV Results:\")\n",
    "            print(f\"  Best CV score: {model_results['cv_results']['best_score']:.4f}\")\n",
    "            print(f\"  Best params: {model_results['cv_results']['best_params']}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # LIFT TABLE (Classification only)\n",
    "    # ========================================================================\n",
    "    if task_type == 'classification':\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"LIFT TABLE ANALYSIS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        MODEL_TO_EXAMINE = cohort_results['best_model_name']  # Or specify manually\n",
    "        \n",
    "        if MODEL_TO_EXAMINE in cohort_results['candidate_results']:\n",
    "            lift_table = cohort_results['candidate_results'][MODEL_TO_EXAMINE]['metrics']['oot']['lift_table']\n",
    "            \n",
    "            print(f\"\\nModel: {MODEL_TO_EXAMINE}\")\n",
    "            print(f\"Cohort: {COHORT_TO_EXAMINE}\")\n",
    "            print(\"\\n\" + lift_table.to_string(index=False))\n",
    "            \n",
    "            # Key insights\n",
    "            print(f\"\\n{'─'*60}\")\n",
    "            print(\"Key Insights:\")\n",
    "            print(f\"{'─'*60}\")\n",
    "            top_decile_lift = lift_table.iloc[0]['lift']\n",
    "            top_decile_capture = lift_table.iloc[0]['cum_pct_captured']\n",
    "            top_5pct = lift_table.iloc[:1]\n",
    "            \n",
    "            print(f\"Top 5% lift: {top_decile_lift:.2f}x\")\n",
    "            print(f\"Top 5% captures: {top_decile_capture:.1%} of all events\")\n",
    "            \n",
    "            # Find where lift drops below 2x\n",
    "            below_2x = lift_table[lift_table['cum_lift'] < 2.0]\n",
    "            if len(below_2x) > 0:\n",
    "                first_below = below_2x.iloc[0]\n",
    "                pct = (first_below['quantile'] / 20) * 100\n",
    "                print(f\"Lift drops below 2x at: top {pct:.0f}%\")\n",
    "        else:\n",
    "            print(f\"⚠️  Model '{MODEL_TO_EXAMINE}' not found\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SHAP ANALYSIS (Best model only)\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SHAP FEATURE IMPORTANCE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if cohort_results['shap_values'] is not None:\n",
    "        shap_dict = cohort_results['shap_values']\n",
    "        \n",
    "        print(f\"\\nModel: {cohort_results['best_model_name']}\")\n",
    "        print(f\"Cohort: {COHORT_TO_EXAMINE}\")\n",
    "        \n",
    "        # Generate SHAP direction summary\n",
    "        shap_direction = summarize_shap_direction(shap_dict, top_n=20)\n",
    "        \n",
    "        if shap_direction is not None:\n",
    "            print(\"\\nTop Features by SHAP Importance:\")\n",
    "            print(shap_direction.to_string(index=False))\n",
    "            \n",
    "            # Plot SHAP summary\n",
    "            print(\"\\nGenerating SHAP summary plot...\")\n",
    "            output_dir = Path(OUTPUT_BASE_DIR) / LABEL / COHORT_TO_EXAMINE / 'best_model' / 'plots'\n",
    "            plot_shap_summary(\n",
    "                shap_dict,\n",
    "                output_dir / 'shap_summary_detailed.png',\n",
    "                top_n=20\n",
    "            )\n",
    "            print(\"✓ SHAP plot saved\")\n",
    "    else:\n",
    "        print(\"⚠️  SHAP values not available (may be disabled or ensemble model)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # THRESHOLD ANALYSIS (Classification only)\n",
    "    # ========================================================================\n",
    "    if task_type == 'classification' and cohort_results['threshold_results']:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "    # ========================================================================\n",
    "    # COEFFICIENT TABLE (Logistic Regression only)\n",
    "    # ========================================================================\n",
    "    if cohort_results['coeff_table'] is not None:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"LOGISTIC REGRESSION COEFFICIENTS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(f\"\\nModel: {cohort_results['best_model_name']}\")\n",
    "        print(f\"Cohort: {COHORT_TO_EXAMINE}\")\n",
    "        \n",
    "        coeff_table = cohort_results['coeff_table']\n",
    "        \n",
    "        print(\"\\nTop Features by Absolute Coefficient:\")\n",
    "        print(coeff_table[['feature', 'coefficient', 'odds_ratio', 'interpretation']].to_string(index=False))\n",
    "        \n",
    "        # Summary stats\n",
    "        print(f\"\\n{'─'*60}\")\n",
    "        print(\"Summary:\")\n",
    "        print(f\"{'─'*60}\")\n",
    "        print(f\"Total features: {len(coeff_table)}\")\n",
    "        \n",
    "        positive_coef = (coeff_table['coefficient'] > 0).sum()\n",
    "        negative_coef = (coeff_table['coefficient'] < 0).sum()\n",
    "        print(f\"Positive coefficients (increase odds): {positive_coef}\")\n",
    "        print(f\"Negative coefficients (decrease odds): {negative_coef}\")\n",
    "        \n",
    "        # Strongest effects\n",
    "        max_positive = coeff_table.loc[coeff_table['coefficient'].idxmax()]\n",
    "        max_negative = coeff_table.loc[coeff_table['coefficient'].idxmin()]\n",
    "        \n",
    "        print(f\"\\nStrongest positive effect: {max_positive['feature']}\")\n",
    "        print(f\"  Odds ratio: {max_positive['odds_ratio']:.3f}\")\n",
    "        print(f\"\\nStrongest negative effect: {max_negative['feature']}\")\n",
    "        print(f\"  Odds ratio: {max_negative['odds_ratio']:.3f}\")\n",
    "    \n",
    "        print(f\"THRESHOLD OPTIMIZATION\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(f\"\\nModel: {cohort_results['best_model_name']}\")\n",
    "        print(f\"Cohort: {COHORT_TO_EXAMINE}\")\n",
    "        \n",
    "        print(\"\\nOptimal Thresholds:\")\n",
    "        for obj_name, obj_result in cohort_results['threshold_results'].items():\n",
    "            if obj_name != '_threshold_curve':\n",
    "                print(f\"\\n{obj_name}:\")\n",
    "                print(f\"  Threshold: {obj_result['threshold']:.3f}\")\n",
    "                for k, v in obj_result.items():\n",
    "                    if k != 'threshold':\n",
    "                        if isinstance(v, float):\n",
    "                            print(f\"  {k}: {v:.4f}\")\n",
    "                        else:\n",
    "                            print(f\"  {k}: {v}\")\n",
    "        \n",
    "        # Show threshold curve stats\n",
    "        if '_threshold_curve' in cohort_results['threshold_results']:\n",
    "            df_curve = cohort_results['threshold_results']['_threshold_curve']\n",
    "            \n",
    "            print(f\"\\n{'─'*60}\")\n",
    "            print(\"Threshold Sensitivity:\")\n",
    "            print(f\"{'─'*60}\")\n",
    "            \n",
    "            # Show key thresholds\n",
    "            thresholds_to_show = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "            print(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "            print(\"─\" * 50)\n",
    "            for thresh in thresholds_to_show:\n",
    "                row = df_curve.iloc[(df_curve['threshold'] - thresh).abs().argmin()]\n",
    "                print(f\"{row['threshold']:<12.2f} {row['precision']:<12.3f} \"\n",
    "                      f\"{row['recall']:<12.3f} {row['f1']:<12.3f}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PREDICTION DISTRIBUTION\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PREDICTION DISTRIBUTION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nModel: {cohort_results['best_model_name']}\")\n",
    "    print(f\"Cohort: {COHORT_TO_EXAMINE}\")\n",
    "    \n",
    "    oot_preds = cohort_results['best_predictions']['oot']\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        pred_col = 'predicted'\n",
    "        print(\"\\nPredicted Probability Distribution:\")\n",
    "        print(f\"  Min:    {oot_preds[pred_col].min():.4f}\")\n",
    "        print(f\"  25th %: {oot_preds[pred_col].quantile(0.25):.4f}\")\n",
    "        print(f\"  Median: {oot_preds[pred_col].median():.4f}\")\n",
    "        print(f\"  75th %: {oot_preds[pred_col].quantile(0.75):.4f}\")\n",
    "        print(f\"  Max:    {oot_preds[pred_col].max():.4f}\")\n",
    "        \n",
    "        # Show actual event rate by score bucket\n",
    "        try:\n",
    "            oot_preds['score_bucket'] = pd.qcut(\n",
    "                oot_preds[pred_col], \n",
    "                q=10, \n",
    "                labels=False,  # Use numeric labels instead\n",
    "                duplicates='drop'\n",
    "            )\n",
    "            \n",
    "            # Convert to decile labels\n",
    "            unique_buckets = sorted(oot_preds['score_bucket'].unique())\n",
    "            bucket_map = {old: f'D{i+1}' for i, old in enumerate(unique_buckets)}\n",
    "            oot_preds['score_bucket'] = oot_preds['score_bucket'].map(bucket_map)\n",
    "            \n",
    "            bucket_stats = oot_preds.groupby('score_bucket').agg({\n",
    "                'actual': ['count', 'mean'],\n",
    "                'predicted': 'mean'\n",
    "            }).round(4)\n",
    "            \n",
    "            print(\"\\nActual Event Rate by Score Bucket:\")\n",
    "            print(f\"(Created {len(unique_buckets)} buckets)\")\n",
    "            print(bucket_stats.to_string())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️  Could not create score buckets: {e}\")\n",
    "            print(\"Scores may have too many duplicate values\")\n",
    "        \n",
    "    else:\n",
    "        pred_col = 'predicted'\n",
    "        print(\"\\nPredicted Value Distribution:\")\n",
    "        print(f\"  Min:    {oot_preds[pred_col].min():.2f}\")\n",
    "        print(f\"  25th %: {oot_preds[pred_col].quantile(0.25):.2f}\")\n",
    "        print(f\"  Median: {oot_preds[pred_col].median():.2f}\")\n",
    "        print(f\"  75th %: {oot_preds[pred_col].quantile(0.75):.2f}\")\n",
    "        print(f\"  Max:    {oot_preds[pred_col].max():.2f}\")\n",
    "        \n",
    "        print(\"\\nActual Value Distribution:\")\n",
    "        print(f\"  Min:    {oot_preds['actual'].min():.2f}\")\n",
    "        print(f\"  25th %: {oot_preds['actual'].quantile(0.25):.2f}\")\n",
    "        print(f\"  Median: {oot_preds['actual'].median():.2f}\")\n",
    "        print(f\"  75th %: {oot_preds['actual'].quantile(0.75):.2f}\")\n",
    "        print(f\"  Max:    {oot_preds['actual'].max():.2f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"END OF DETAILED EXAMINATION\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "1. **Review metrics**: Check the summary table above\n",
    "2. **Inspect features**: Look at top SHAP features\n",
    "3. **Examine plots**: Open the plots directory for visualizations\n",
    "4. **Use model**: Load the saved model.pkl to score new donors\n",
    "5. **Iterate**: Adjust configs and re-run with different labels/cohorts\n",
    "\n",
    "### To score new donors:\n",
    "```python\n",
    "# Load best model\n",
    "with open('./models/<label>/<cohort>/best_model/model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Load config to get feature names\n",
    "with open('./models/<label>/<cohort>/best_model/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Score new data (must have same features)\n",
    "new_data = pd.read_csv('new_donors.csv')\n",
    "new_data_features = new_data[config['feature_names']]\n",
    "predictions = model.predict_proba(new_data_features)[:, 1]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
